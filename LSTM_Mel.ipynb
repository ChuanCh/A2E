{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import models\n",
    "from utils import *\n",
    "import scipy.signal\n",
    "import importlib\n",
    "importlib.reload(models)\n",
    "from models import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "samplerate = 16000\n",
    "num_epochs = 300\n",
    "hidden_size = 256\n",
    "second_hidden_size = 256\n",
    "third_hidden_size = 256\n",
    "learning_rate = 0.001\n",
    "# the f0 range is from 50 to 500 Hz, it should cover the longest f0 in the dataset, so 50Hz=0.02s\n",
    "segment_length_in_seconds = 0.02\n",
    "# noise level\n",
    "segment_length_in_samples = int(segment_length_in_seconds * samplerate)\n",
    "batch_size = 1\n",
    "representation_type = 'Mel'\n",
    "\n",
    "# Parameters for the band-pass filter\n",
    "lowcut = 50.0  # Low frequency cut-off in Hz\n",
    "highcut = 3000.0  # High frequency cut-off in Hz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all wav files in audio\n",
    "audio = []\n",
    "for root, dirs, files in os.walk('audio'):\n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            audio.append(os.path.join(root, file))\n",
    "raw_wav = []\n",
    "\n",
    "# the audio is 2 channel audio, concatenate them along the time axis\n",
    "for i in range(len(audio)):\n",
    "    wav, _ = librosa.load(audio[i], sr=samplerate, mono=False)\n",
    "    raw_wav = np.concatenate((raw_wav, wav), axis=-1) if i != 0 else wav\n",
    "\n",
    "print(raw_wav.shape)\n",
    "\n",
    "\n",
    "input_wav = raw_wav[0]\n",
    "target_wav = raw_wav[1]\n",
    "\n",
    "# plot the wav\n",
    "plt.figure()\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(input_wav)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(target_wav)\n",
    "plt.show()\n",
    "\n",
    "# convert the raw wav to mel spectrogram\n",
    "mel_input = librosa.power_to_db(librosa.feature.melspectrogram(y=input_wav, sr=samplerate, n_mels=32, n_fft=1024, hop_length=256, win_length=None))\n",
    "mel_target = librosa.power_to_db(librosa.feature.melspectrogram(y=target_wav, sr=samplerate, n_mels=32, n_fft=1024, hop_length=256, win_length=None))\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reshape_mel_spectrogram(mel_input, num_frames):\n",
    "    \"\"\"\n",
    "    Reshape a Mel spectrogram from (n_mels, total_samples) to (num_samples, num_frames, n_mels).\n",
    "\n",
    "    Args:\n",
    "        mel_input (numpy.ndarray): The Mel spectrogram with shape (n_mels, total_samples).\n",
    "        num_frames (int): The desired number of frames (time steps) for each sample.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The reshaped Mel spectrogram with shape (num_samples, num_frames, n_mels).\n",
    "    \"\"\"\n",
    "    n_mels, total_samples = mel_input.shape\n",
    "    # Ensure that the total number of samples is divisible by the number of frames, if not, delete the last frame\n",
    "    mel_input = mel_input[:, :-(total_samples % num_frames)]\n",
    "\n",
    "    # Calculate the number of samples we'll end up with after the reshape\n",
    "    num_samples = total_samples // num_frames\n",
    "\n",
    "    # Reshape the Mel spectrogram\n",
    "    reshaped_mel_input = mel_input.T.reshape(num_samples, num_frames, n_mels)\n",
    "\n",
    "    return reshaped_mel_input\n",
    "    \n",
    "\n",
    "num_frames = 20  # This is an example, adjust to your actual desired frame count\n",
    "reshaped_mel_input = reshape_mel_spectrogram(mel_input, num_frames)\n",
    "reshaped_mel_target = reshape_mel_spectrogram(mel_target, num_frames)\n",
    "\n",
    "# plot the mel spectrogram\n",
    "plt.figure()\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(mel_input, aspect='auto', origin='lower')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(mel_target, aspect='auto', origin='lower')\n",
    "plt.show()\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, input_mel_specs, target_mel_specs):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with input and target Mel spectrograms.\n",
    "\n",
    "        Args:\n",
    "        input_mel_specs (numpy.ndarray): A 3D numpy array of shape (num_samples, num_frames, n_mels)\n",
    "                                         containing the Mel spectrogram for the input audio.\n",
    "        target_mel_specs (numpy.ndarray): A 3D numpy array of shape (num_samples, num_frames, n_mels)\n",
    "                                          containing the Mel spectrogram for the target audio.\n",
    "        \"\"\"\n",
    "        assert input_mel_specs.shape == target_mel_specs.shape, \"Input and target spectrograms must have the same shape\"\n",
    "        \n",
    "        self.input_mel_specs = input_mel_specs\n",
    "        self.target_mel_specs = target_mel_specs\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return self.input_mel_specs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the input and target Mel spectrogram for a given index.\n",
    "\n",
    "        Args:\n",
    "        idx (int): The index of the sample.\n",
    "\n",
    "        Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: The input and target Mel spectrograms as PyTorch tensors.\n",
    "        \"\"\"\n",
    "        input_spec = torch.tensor(self.input_mel_specs[idx], dtype=torch.float32)\n",
    "        target_spec = torch.tensor(self.target_mel_specs[idx], dtype=torch.float32)\n",
    "        \n",
    "        return input_spec, target_spec\n",
    "\n",
    "# Assuming input_mel_specs and target_mel_specs are your datasets\n",
    "dataset = VoiceDataset(reshaped_mel_input, reshaped_mel_target)\n",
    "\n",
    "# split the dataset into training and validation sets and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# shuffle the dataset\n",
    "torch.manual_seed(0)\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMmodel(n_mels=32, hidden_size=hidden_size, num_layers=2)\n",
    "log_name = f'{type(model).__name__}_{representation_type}_bs{batch_size}'\n",
    "writer = SummaryWriter('runs/experiment_name')\n",
    "\n",
    "sample_input = torch.randn(batch_size, segment_length_in_samples)\n",
    "# writer.add_graph(model, sample_input)\n",
    "\n",
    "tensorboard_log_config(writer, model, representation_type, samplerate, segment_length_in_seconds, batch_size)\n",
    "trained_model = train_model(writer, model, train_loader, val_loader, num_epochs, learning_rate)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "predictions = []  # To store model predictions\n",
    "actuals = []  # To store actual target values for comparison\n",
    "audios = []  # To store audio samples\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "    for inputs, targets in test_loader:\n",
    "        # Move data to the appropriate device (e.g., GPU if available)\n",
    "        inputs = inputs.to(next(model.parameters()).device)\n",
    "        \n",
    "        # Compute model output\n",
    "        output = model(inputs)\n",
    "        \n",
    "        # Store predictions and actuals and inputs\n",
    "        audios.append(inputs.cpu().numpy())\n",
    "        predictions.append(output.cpu().numpy())  # Move prediction back to CPU and convert to numpy\n",
    "        actuals.append(targets.cpu().numpy())  # Same for actual targets\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(20, 10))\n",
    "\n",
    "# Plot the input waveform\n",
    "axs[0].imshow(audios[10].T, aspect='auto', origin='lower')\n",
    "axs[0].set_title('Input audio Mel Spectrogram')\n",
    "\n",
    "# Plot actual Mel spectrogram\n",
    "axs[1].imshow(actuals[10].T, aspect='auto', origin='lower')\n",
    "axs[1].set_title('Actual EGG Mel Spectrogram')\n",
    "\n",
    "# Plot predicted Mel spectrogram\n",
    "axs[2].imshow(predictions[10].T, aspect='auto', origin='lower')\n",
    "axs[2].set_title('Predicted EGG Mel Spectrogram')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the mel spectrogram back to wav for target and output\n",
    "mel_audio_test = audios[0].T\n",
    "mel_output_test = predictions[0].T\n",
    "mel_target_test = actuals[0].T\n",
    "\n",
    "# return the Mel spectrogram back to its original 2D shape\n",
    "mel_audio_test = mel_audio_test.reshape(mel_audio_test.shape[0], mel_audio_test.shape[1])\n",
    "mel_output_test = mel_output_test.reshape(mel_output_test.shape[0], mel_output_test.shape[1])\n",
    "mel_target_test = mel_target_test.reshape(mel_target_test.shape[0], mel_target_test.shape[1])\n",
    "\n",
    "# convert the mel spectrogram back to wav\n",
    "input_wav_test = librosa.feature.inverse.mel_to_audio(mel_audio_test, sr=samplerate, n_fft=1024, hop_length=256, win_length=None)\n",
    "output_wav_test = librosa.feature.inverse.mel_to_audio(mel_output_test, sr=samplerate, n_fft=1024, hop_length=256, win_length=None)\n",
    "target_wav_test = librosa.feature.inverse.mel_to_audio(mel_target_test, sr=samplerate, n_fft=1024, hop_length=256, win_length=None)\n",
    "\n",
    "# plot the input, the target, the prediction\n",
    "plt.figure()\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(input_wav_test)\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(output_wav_test)\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(target_wav_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the output wav\n",
    "# librosa.output.write_wav('output.wav', output_wav_test, samplerate)\n",
    "# librosa.output.write_wav('target.wav', target_wav_test, samplerate)\n",
    "# librosa.output.write_wav('input.wav', input_wav_test, samplerate)\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
