{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use magnitudes and phases resulting from DFT as input.\n",
    "Input size now x2.\n",
    "phase needs to be unwrapped\n",
    "try conv layer hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From g:\\Conda\\envs\\pytorch\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import models\n",
    "import utils\n",
    "import scipy.signal\n",
    "import importlib\n",
    "importlib.reload(models)\n",
    "importlib.reload(utils)\n",
    "from models import *\n",
    "from utils import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "samplerate = 44100\n",
    "num_epochs = 100\n",
    "hidden_size = 256\n",
    "second_hidden_size = 256\n",
    "third_hidden_size = 256\n",
    "learning_rate = 0.001\n",
    "# the f0 range is from 50 to 500 Hz, it should cover the longest f0 in the dataset, so 50Hz=0.02s\n",
    "segment_length_in_seconds = 0.02\n",
    "# noise level\n",
    "segment_length_in_samples = int(segment_length_in_seconds * samplerate)\n",
    "batch_size = 1\n",
    "representation_type = 'Mel'\n",
    "\n",
    "# STFT parameters\n",
    "n_fft = 2048\n",
    "hop_length = 1024\n",
    "window = 'hann'\n",
    "num_frames = 20\n",
    "n_harmonics = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VoiceDataset.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m (audio_train, egg_train), (audio_val, egg_val), (audio_test, egg_test) \u001b[38;5;241m=\u001b[39m split_data(input_complex, target_complex, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# create the dataset\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mVoiceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43megg_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m VoiceDataset(audio_val, egg_val)\n\u001b[0;32m     30\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m VoiceDataset(audio_test, egg_test)\n",
      "\u001b[1;31mTypeError\u001b[0m: VoiceDataset.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# load all wav files in audio\n",
    "audio = []\n",
    "for root, dirs, files in os.walk('audio'):\n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            audio.append(os.path.join(root, file))\n",
    "raw_wav = []\n",
    "\n",
    "# the audio is 2 channel audio, concatenate them along the time axis\n",
    "for i in range(len(audio)):\n",
    "    wav, _ = librosa.load(audio[i], sr=samplerate, mono=False)\n",
    "    raw_wav = np.concatenate((raw_wav, wav), axis=-1) if i != 0 else wav\n",
    "\n",
    "audio_wav = raw_wav[0]\n",
    "egg_wav = raw_wav[1]\n",
    "\n",
    "# preprocess the audio_wav and egg_wav\n",
    "input_complex, target_complex, mag_min, mag_max = preprocess_data(audio_wav, egg_wav, n_fft, hop_length, n_harmonics, window)\n",
    "\n",
    "# reshape the input_complex and target_complex\n",
    "input_complex = reshape_to_fit_LSTM(input_complex, num_frames)\n",
    "target_complex = reshape_to_fit_LSTM(target_complex, num_frames)\n",
    "\n",
    "# split the data into train, validation and test set\n",
    "(audio_train, egg_train), (audio_val, egg_val), (audio_test, egg_test) = split_data(input_complex, target_complex, 0.8, 0.1)\n",
    "\n",
    "# create the dataset\n",
    "train_dataset = VoiceDataset(audio_train, egg_train)\n",
    "val_dataset = VoiceDataset(audio_val, egg_val)\n",
    "test_dataset = VoiceDataset(audio_test, egg_test)\n",
    "\n",
    "# create the dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = (n_fft//2 +1) * 3\n",
    "model = LSTMmodel(n_mels=n_mels, hidden_size=hidden_size, num_layers=2)\n",
    "log_name = f'{type(model).__name__}_{representation_type}_bs{batch_size}'\n",
    "writer = SummaryWriter('runs/experiment_name')\n",
    "\n",
    "sample_input = torch.randn(batch_size, segment_length_in_samples)\n",
    "# writer.add_graph(model, sample_input)\n",
    "\n",
    "tensorboard_log_config(writer, model, representation_type, samplerate, segment_length_in_seconds, batch_size)\n",
    "trained_model = train_model(writer, model, train_loader, val_loader, num_epochs, learning_rate)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "predictions = []  # To store model predictions\n",
    "actuals = []  # To store actual target values for comparison\n",
    "audios = []  # To store audio samples\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "    for inputs, targets in test_loader:\n",
    "        # Move data to the appropriate device (e.g., GPU if available)\n",
    "        inputs = inputs.to(next(model.parameters()).device)\n",
    "        \n",
    "        # Compute model output\n",
    "        output = model(inputs)\n",
    "        \n",
    "        predictions.append(output.cpu().numpy())  # Move prediction back to CPU and convert to numpy\n",
    "        actuals.append(targets.cpu().numpy())  # Same for actual targets\n",
    "\n",
    "for prediction in predictions:\n",
    "    # Split the predictions into magnitude and sin and cos of phase\n",
    "    pred_mag = prediction[:, :, :n_mels]\n",
    "    pred_phase_sin = prediction[:, :, n_mels:2*n_mels]\n",
    "    pred_phase_cos = prediction[:, :, 2*n_mels:]\n",
    "        \n",
    "    # Denormalize magnitude and phase\n",
    "    pred_mag_denorm = denormalize(pred_mag, mag_min, mag_max)\n",
    "    \n",
    "    # reconstruct audio from magnitude and sin and cos of phase\n",
    "    for mag, phase in zip(pred_mag_denorm, zip(pred_phase_sin, pred_phase_cos)):\n",
    "        audio = reconstruct_audio(mag, phase, n_fft, hop_length, window)\n",
    "        audios.append(audio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the input waveform, actual EGG spectrogram, and predicted EGG spectrogram for a random sample\n",
    "idx = random.randint(0, len(predictions) - 1)\n",
    "input_mel = reshaped_mel_input_test[idx, :, :]\n",
    "target_mel = reshaped_mel_target_test[idx, :, :]\n",
    "prediction_mel = predictions[idx]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.imshow(np.abs(input_mel.T), aspect='auto', origin='lower', cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.title('Input Mel spectrogram')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(np.abs(target_mel.T), aspect='auto', origin='lower', cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.title('Target Mel spectrogram')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(np.abs(prediction_mel.T), aspect='auto', origin='lower', cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.title('Predicted Mel spectrogram')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the input waveform, actual EGG waveform, and predicted EGG waveform for a random sample\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(audio_wav_test[idx])\n",
    "plt.title('Input waveform')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(egg_wav_test[idx])\n",
    "plt.title('Target waveform')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(audios[idx])\n",
    "plt.title('Predicted waveform')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
