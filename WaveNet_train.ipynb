{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Conda\\envs\\HiFi\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, Loss: 0.12196299433708191\n",
      "Epoch 0, Iteration 10, Loss: 0.06552340090274811\n",
      "Epoch 0, Iteration 20, Loss: 0.0631914958357811\n",
      "Epoch 0, Iteration 30, Loss: 0.03090805560350418\n",
      "Epoch 0, Iteration 40, Loss: 0.08880951255559921\n",
      "Epoch 0, Iteration 50, Loss: 0.041820697486400604\n",
      "Epoch 0, Iteration 60, Loss: 0.08387257903814316\n",
      "Epoch 0, Iteration 70, Loss: 0.044668976217508316\n",
      "Epoch 0, Iteration 80, Loss: 0.04732251912355423\n",
      "Epoch 0, Iteration 90, Loss: 0.04108284413814545\n",
      "Epoch 0, Iteration 100, Loss: 0.0571928508579731\n",
      "Epoch 0, Iteration 110, Loss: 0.027505138888955116\n",
      "Epoch 0, Iteration 120, Loss: 0.02444513887166977\n",
      "Epoch 0, Iteration 130, Loss: 0.03135203570127487\n",
      "Epoch 0, Iteration 140, Loss: 0.03350323438644409\n",
      "Epoch 0, Iteration 150, Loss: 0.0559917576611042\n",
      "Epoch 0, Iteration 160, Loss: 0.04439442232251167\n",
      "Epoch 0, Iteration 170, Loss: 0.028451699763536453\n",
      "Epoch 0, Iteration 180, Loss: 0.04317871481180191\n",
      "Epoch 0, Iteration 190, Loss: 0.04030776396393776\n",
      "Epoch 0, Iteration 200, Loss: 0.05597873404622078\n",
      "Epoch 0, Iteration 210, Loss: 0.03785526379942894\n",
      "Epoch 0, Iteration 220, Loss: 0.055155426263809204\n",
      "Epoch 0, Iteration 230, Loss: 0.04213191941380501\n",
      "Epoch 0, Iteration 240, Loss: 0.07866635173559189\n",
      "Epoch 0, Iteration 250, Loss: 0.043238408863544464\n",
      "Epoch 0, Iteration 260, Loss: 0.08553743362426758\n",
      "Epoch 0, Iteration 270, Loss: 0.07797342538833618\n",
      "Epoch 0, Iteration 280, Loss: 0.02756595052778721\n",
      "Epoch 0, Iteration 290, Loss: 0.02565922774374485\n",
      "Epoch 0, Iteration 300, Loss: 0.0373799130320549\n",
      "Epoch 0, Iteration 310, Loss: 0.020327778533101082\n",
      "Epoch 0, Iteration 320, Loss: 0.044184017926454544\n",
      "Epoch 0, Iteration 330, Loss: 0.024276433512568474\n",
      "Epoch 0, Iteration 340, Loss: 0.08920695632696152\n",
      "Epoch 0, Iteration 350, Loss: 0.0578589104115963\n",
      "Epoch 0, Iteration 360, Loss: 0.08745965361595154\n",
      "Epoch 0, Iteration 370, Loss: 0.06417252123355865\n",
      "Epoch 0, Iteration 380, Loss: 0.045051202178001404\n",
      "Epoch 0, Iteration 390, Loss: 0.10409091413021088\n",
      "Epoch 0, Iteration 400, Loss: 0.0955989807844162\n",
      "Epoch 0, Iteration 410, Loss: 0.024727413430809975\n",
      "Epoch 0, Iteration 420, Loss: 0.04826086759567261\n",
      "Epoch 0, Iteration 430, Loss: 0.03952343761920929\n",
      "Epoch 0, Iteration 440, Loss: 0.06605439633131027\n",
      "Epoch 0, Iteration 450, Loss: 0.0569501556456089\n",
      "Epoch 0, Iteration 460, Loss: 0.058380212634801865\n",
      "Epoch 0, Iteration 470, Loss: 0.02945300005376339\n",
      "Epoch 0, Iteration 480, Loss: 0.02584720402956009\n",
      "Epoch 0, Iteration 490, Loss: 0.04941607639193535\n",
      "Epoch 0, Iteration 500, Loss: 0.08186274021863937\n",
      "Epoch 0, Iteration 510, Loss: 0.03046729601919651\n",
      "Epoch 0, Iteration 520, Loss: 0.03132695332169533\n",
      "Epoch 0, Iteration 530, Loss: 0.03214431554079056\n",
      "Epoch 0, Iteration 540, Loss: 0.03448691591620445\n",
      "Epoch 0, Iteration 550, Loss: 0.055009521543979645\n",
      "Epoch 0, Iteration 560, Loss: 0.08906908333301544\n",
      "Epoch 0, Iteration 570, Loss: 0.029435917735099792\n",
      "Epoch 0, Iteration 580, Loss: 0.012464435771107674\n",
      "Epoch 0, Iteration 590, Loss: 0.07445219904184341\n",
      "Epoch 0, Iteration 600, Loss: 0.04371894896030426\n",
      "Epoch 0, Iteration 610, Loss: 0.019378427416086197\n",
      "Epoch 0, Iteration 620, Loss: 0.028223687782883644\n",
      "Epoch 0, Iteration 630, Loss: 0.04109016805887222\n",
      "Epoch 0, Iteration 640, Loss: 0.06687338650226593\n",
      "Epoch 0, Iteration 650, Loss: 0.02844119444489479\n",
      "Epoch 0, Iteration 660, Loss: 0.05333048477768898\n",
      "Epoch 0, Iteration 670, Loss: 0.12527598440647125\n",
      "Epoch 0, Iteration 680, Loss: 0.05207446217536926\n",
      "Epoch 0, Iteration 690, Loss: 0.03825745731592178\n",
      "Epoch 0, Iteration 700, Loss: 0.05032997578382492\n",
      "Epoch 0, Iteration 710, Loss: 0.12185975909233093\n",
      "Epoch 0, Iteration 720, Loss: 0.05231126397848129\n",
      "Epoch 0, Iteration 730, Loss: 0.07753008604049683\n",
      "Epoch 0, Validation Loss: 0.04542857967317104\n",
      "Validation loss decreased (inf --> 0.045429). Saving model...\n",
      "Epoch 1, Iteration 0, Loss: 0.047402020543813705\n",
      "Epoch 1, Iteration 10, Loss: 0.043911222368478775\n",
      "Epoch 1, Iteration 20, Loss: 0.04162003844976425\n",
      "Epoch 1, Iteration 30, Loss: 0.0325680673122406\n",
      "Epoch 1, Iteration 40, Loss: 0.07752390205860138\n",
      "Epoch 1, Iteration 50, Loss: 0.02637718990445137\n",
      "Epoch 1, Iteration 60, Loss: 0.028625480830669403\n",
      "Epoch 1, Iteration 70, Loss: 0.050218913704156876\n",
      "Epoch 1, Iteration 80, Loss: 0.0493689626455307\n",
      "Epoch 1, Iteration 90, Loss: 0.028368309140205383\n",
      "Epoch 1, Iteration 100, Loss: 0.12506017088890076\n",
      "Epoch 1, Iteration 110, Loss: 0.026089290156960487\n",
      "Epoch 1, Iteration 120, Loss: 0.021853815764188766\n",
      "Epoch 1, Iteration 130, Loss: 0.024003561586141586\n",
      "Epoch 1, Iteration 140, Loss: 0.08480466902256012\n",
      "Epoch 1, Iteration 150, Loss: 0.04920375347137451\n",
      "Epoch 1, Iteration 160, Loss: 0.022704394534230232\n",
      "Epoch 1, Iteration 170, Loss: 0.0727878138422966\n",
      "Epoch 1, Iteration 180, Loss: 0.0684608742594719\n",
      "Epoch 1, Iteration 190, Loss: 0.0332811176776886\n",
      "Epoch 1, Iteration 200, Loss: 0.026270808652043343\n",
      "Epoch 1, Iteration 210, Loss: 0.05996569246053696\n",
      "Epoch 1, Iteration 220, Loss: 0.047670669853687286\n",
      "Epoch 1, Iteration 230, Loss: 0.04953437298536301\n",
      "Epoch 1, Iteration 240, Loss: 0.034583549946546555\n",
      "Epoch 1, Iteration 250, Loss: 0.04902372881770134\n",
      "Epoch 1, Iteration 260, Loss: 0.03334186226129532\n",
      "Epoch 1, Iteration 270, Loss: 0.04917258024215698\n",
      "Epoch 1, Iteration 280, Loss: 0.03896011784672737\n",
      "Epoch 1, Iteration 290, Loss: 0.05112502723932266\n",
      "Epoch 1, Iteration 300, Loss: 0.020869053900241852\n",
      "Epoch 1, Iteration 310, Loss: 0.03441724181175232\n",
      "Epoch 1, Iteration 320, Loss: 0.02627933956682682\n",
      "Epoch 1, Iteration 330, Loss: 0.09252126514911652\n",
      "Epoch 1, Iteration 340, Loss: 0.03743867203593254\n",
      "Epoch 1, Iteration 350, Loss: 0.025362536311149597\n",
      "Epoch 1, Iteration 360, Loss: 0.07323358207941055\n",
      "Epoch 1, Iteration 370, Loss: 0.039713114500045776\n",
      "Epoch 1, Iteration 380, Loss: 0.0340668186545372\n",
      "Epoch 1, Iteration 390, Loss: 0.05056383088231087\n",
      "Epoch 1, Iteration 400, Loss: 0.028928613290190697\n",
      "Epoch 1, Iteration 410, Loss: 0.04041751101613045\n",
      "Epoch 1, Iteration 420, Loss: 0.048688776791095734\n",
      "Epoch 1, Iteration 430, Loss: 0.0340762622654438\n",
      "Epoch 1, Iteration 440, Loss: 0.038475021719932556\n",
      "Epoch 1, Iteration 450, Loss: 0.05400313064455986\n",
      "Epoch 1, Iteration 460, Loss: 0.039027463644742966\n",
      "Epoch 1, Iteration 470, Loss: 0.07156810164451599\n",
      "Epoch 1, Iteration 480, Loss: 0.06136728450655937\n",
      "Epoch 1, Iteration 490, Loss: 0.03710748255252838\n",
      "Epoch 1, Iteration 500, Loss: 0.04092639684677124\n",
      "Epoch 1, Iteration 510, Loss: 0.055556125938892365\n",
      "Epoch 1, Iteration 520, Loss: 0.06302418559789658\n",
      "Epoch 1, Iteration 530, Loss: 0.04632535204291344\n",
      "Epoch 1, Iteration 540, Loss: 0.041718218475580215\n",
      "Epoch 1, Iteration 550, Loss: 0.03731536120176315\n",
      "Epoch 1, Iteration 560, Loss: 0.018860870972275734\n",
      "Epoch 1, Iteration 570, Loss: 0.0770544782280922\n",
      "Epoch 1, Iteration 580, Loss: 0.04334156960248947\n",
      "Epoch 1, Iteration 590, Loss: 0.04266265034675598\n",
      "Epoch 1, Iteration 600, Loss: 0.044691719114780426\n",
      "Epoch 1, Iteration 610, Loss: 0.04450676590204239\n",
      "Epoch 1, Iteration 620, Loss: 0.05890864133834839\n",
      "Epoch 1, Iteration 630, Loss: 0.046939313411712646\n",
      "Epoch 1, Iteration 640, Loss: 0.05177876353263855\n",
      "Epoch 1, Iteration 650, Loss: 0.024121198803186417\n",
      "Epoch 1, Iteration 660, Loss: 0.023900769650936127\n",
      "Epoch 1, Iteration 670, Loss: 0.04260267689824104\n",
      "Epoch 1, Iteration 680, Loss: 0.021214451640844345\n",
      "Epoch 1, Iteration 690, Loss: 0.027707165107131004\n",
      "Epoch 1, Iteration 700, Loss: 0.03996039181947708\n",
      "Epoch 1, Iteration 710, Loss: 0.03892222046852112\n",
      "Epoch 1, Iteration 720, Loss: 0.019633665680885315\n",
      "Epoch 1, Iteration 730, Loss: 0.07156281918287277\n",
      "Epoch 1, Validation Loss: 0.04475325797482029\n",
      "Validation loss decreased (0.045429 --> 0.044753). Saving model...\n",
      "Epoch 2, Iteration 0, Loss: 0.10447119176387787\n",
      "Epoch 2, Iteration 10, Loss: 0.0374826118350029\n",
      "Epoch 2, Iteration 20, Loss: 0.041105516254901886\n",
      "Epoch 2, Iteration 30, Loss: 0.05104963108897209\n",
      "Epoch 2, Iteration 40, Loss: 0.03134729713201523\n",
      "Epoch 2, Iteration 50, Loss: 0.019195476546883583\n",
      "Epoch 2, Iteration 60, Loss: 0.06565423309803009\n",
      "Epoch 2, Iteration 70, Loss: 0.06489613652229309\n",
      "Epoch 2, Iteration 80, Loss: 0.04456865042448044\n",
      "Epoch 2, Iteration 90, Loss: 0.07432051748037338\n",
      "Epoch 2, Iteration 100, Loss: 0.03557521849870682\n",
      "Epoch 2, Iteration 110, Loss: 0.041975174099206924\n",
      "Epoch 2, Iteration 120, Loss: 0.020252129063010216\n",
      "Epoch 2, Iteration 130, Loss: 0.04906464368104935\n",
      "Epoch 2, Iteration 140, Loss: 0.02084091305732727\n",
      "Epoch 2, Iteration 150, Loss: 0.04216296970844269\n",
      "Epoch 2, Iteration 160, Loss: 0.029256829991936684\n",
      "Epoch 2, Iteration 170, Loss: 0.06291480362415314\n",
      "Epoch 2, Iteration 180, Loss: 0.04150259867310524\n",
      "Epoch 2, Iteration 190, Loss: 0.05695473402738571\n",
      "Epoch 2, Iteration 200, Loss: 0.018252357840538025\n",
      "Epoch 2, Iteration 210, Loss: 0.04918314516544342\n",
      "Epoch 2, Iteration 220, Loss: 0.04504992440342903\n",
      "Epoch 2, Iteration 230, Loss: 0.022367902100086212\n",
      "Epoch 2, Iteration 240, Loss: 0.0416589081287384\n",
      "Epoch 2, Iteration 250, Loss: 0.0447969064116478\n",
      "Epoch 2, Iteration 260, Loss: 0.07868398725986481\n",
      "Epoch 2, Iteration 270, Loss: 0.023023033514618874\n",
      "Epoch 2, Iteration 280, Loss: 0.04967474564909935\n",
      "Epoch 2, Iteration 290, Loss: 0.06794379651546478\n",
      "Epoch 2, Iteration 300, Loss: 0.03896156698465347\n",
      "Epoch 2, Iteration 310, Loss: 0.048401933163404465\n",
      "Epoch 2, Iteration 320, Loss: 0.045560553669929504\n",
      "Epoch 2, Iteration 330, Loss: 0.03249133378267288\n",
      "Epoch 2, Iteration 340, Loss: 0.07568193972110748\n",
      "Epoch 2, Iteration 350, Loss: 0.08595947921276093\n",
      "Epoch 2, Iteration 360, Loss: 0.04495003819465637\n",
      "Epoch 2, Iteration 370, Loss: 0.04085714742541313\n",
      "Epoch 2, Iteration 380, Loss: 0.12877480685710907\n",
      "Epoch 2, Iteration 390, Loss: 0.08155760914087296\n",
      "Epoch 2, Iteration 400, Loss: 0.048195257782936096\n",
      "Epoch 2, Iteration 410, Loss: 0.03755679354071617\n",
      "Epoch 2, Iteration 420, Loss: 0.035847555845975876\n",
      "Epoch 2, Iteration 430, Loss: 0.046060193330049515\n",
      "Epoch 2, Iteration 440, Loss: 0.021458525210618973\n",
      "Epoch 2, Iteration 450, Loss: 0.023454735055565834\n",
      "Epoch 2, Iteration 460, Loss: 0.034615807235240936\n",
      "Epoch 2, Iteration 470, Loss: 0.039637498557567596\n",
      "Epoch 2, Iteration 480, Loss: 0.055108215659856796\n",
      "Epoch 2, Iteration 490, Loss: 0.0625157281756401\n",
      "Epoch 2, Iteration 500, Loss: 0.0367305725812912\n",
      "Epoch 2, Iteration 510, Loss: 0.02474183589220047\n",
      "Epoch 2, Iteration 520, Loss: 0.05983366444706917\n",
      "Epoch 2, Iteration 530, Loss: 0.04841599613428116\n",
      "Epoch 2, Iteration 540, Loss: 0.07660555094480515\n",
      "Epoch 2, Iteration 550, Loss: 0.03680875897407532\n",
      "Epoch 2, Iteration 560, Loss: 0.05423036962747574\n",
      "Epoch 2, Iteration 570, Loss: 0.054261986166238785\n",
      "Epoch 2, Iteration 580, Loss: 0.040211133658885956\n",
      "Epoch 2, Iteration 590, Loss: 0.05798068270087242\n",
      "Epoch 2, Iteration 600, Loss: 0.0594862699508667\n",
      "Epoch 2, Iteration 610, Loss: 0.040693603456020355\n",
      "Epoch 2, Iteration 620, Loss: 0.0795207992196083\n",
      "Epoch 2, Iteration 630, Loss: 0.042104918509721756\n",
      "Epoch 2, Iteration 640, Loss: 0.04431741684675217\n",
      "Epoch 2, Iteration 650, Loss: 0.03676583245396614\n",
      "Epoch 2, Iteration 660, Loss: 0.027509216219186783\n",
      "Epoch 2, Iteration 670, Loss: 0.04658978804945946\n",
      "Epoch 2, Iteration 680, Loss: 0.037192441523075104\n",
      "Epoch 2, Iteration 690, Loss: 0.04291920363903046\n",
      "Epoch 2, Iteration 700, Loss: 0.06966063380241394\n",
      "Epoch 2, Iteration 710, Loss: 0.031198693439364433\n",
      "Epoch 2, Iteration 720, Loss: 0.032758135348558426\n",
      "Epoch 2, Iteration 730, Loss: 0.04915570095181465\n",
      "Epoch 2, Validation Loss: 0.0439287104603389\n",
      "Validation loss decreased (0.044753 --> 0.043929). Saving model...\n",
      "Epoch 3, Iteration 0, Loss: 0.02574111893773079\n",
      "Epoch 3, Iteration 10, Loss: 0.039244864135980606\n",
      "Epoch 3, Iteration 20, Loss: 0.06902416795492172\n",
      "Epoch 3, Iteration 30, Loss: 0.04078734666109085\n",
      "Epoch 3, Iteration 40, Loss: 0.08000174909830093\n",
      "Epoch 3, Iteration 50, Loss: 0.07824184000492096\n",
      "Epoch 3, Iteration 60, Loss: 0.03670436888933182\n",
      "Epoch 3, Iteration 70, Loss: 0.05554511398077011\n",
      "Epoch 3, Iteration 80, Loss: 0.0968165248632431\n",
      "Epoch 3, Iteration 90, Loss: 0.04128186032176018\n",
      "Epoch 3, Iteration 100, Loss: 0.04893650859594345\n",
      "Epoch 3, Iteration 110, Loss: 0.047589171677827835\n",
      "Epoch 3, Iteration 120, Loss: 0.06216301769018173\n",
      "Epoch 3, Iteration 130, Loss: 0.06997990608215332\n",
      "Epoch 3, Iteration 140, Loss: 0.06370653212070465\n",
      "Epoch 3, Iteration 150, Loss: 0.03243327885866165\n",
      "Epoch 3, Iteration 160, Loss: 0.04934922233223915\n",
      "Epoch 3, Iteration 170, Loss: 0.03225602209568024\n",
      "Epoch 3, Iteration 180, Loss: 0.03863346576690674\n",
      "Epoch 3, Iteration 190, Loss: 0.05064452812075615\n",
      "Epoch 3, Iteration 200, Loss: 0.10196568071842194\n",
      "Epoch 3, Iteration 210, Loss: 0.056930527091026306\n",
      "Epoch 3, Iteration 220, Loss: 0.031582340598106384\n",
      "Epoch 3, Iteration 230, Loss: 0.04714920371770859\n",
      "Epoch 3, Iteration 240, Loss: 0.04850804805755615\n",
      "Epoch 3, Iteration 250, Loss: 0.020949004217982292\n",
      "Epoch 3, Iteration 260, Loss: 0.04489288106560707\n",
      "Epoch 3, Iteration 270, Loss: 0.05779862403869629\n",
      "Epoch 3, Iteration 280, Loss: 0.03606501221656799\n",
      "Epoch 3, Iteration 290, Loss: 0.04600997641682625\n",
      "Epoch 3, Iteration 300, Loss: 0.056938204914331436\n",
      "Epoch 3, Iteration 310, Loss: 0.06838349997997284\n",
      "Epoch 3, Iteration 320, Loss: 0.03247064724564552\n",
      "Epoch 3, Iteration 330, Loss: 0.05961909145116806\n",
      "Epoch 3, Iteration 340, Loss: 0.03519478812813759\n",
      "Epoch 3, Iteration 350, Loss: 0.09213587641716003\n",
      "Epoch 3, Iteration 360, Loss: 0.04587646946310997\n",
      "Epoch 3, Iteration 370, Loss: 0.04776789993047714\n",
      "Epoch 3, Iteration 380, Loss: 0.02923540584743023\n",
      "Epoch 3, Iteration 390, Loss: 0.06134754419326782\n",
      "Epoch 3, Iteration 400, Loss: 0.03044041059911251\n",
      "Epoch 3, Iteration 410, Loss: 0.02207051031291485\n",
      "Epoch 3, Iteration 420, Loss: 0.03664102032780647\n",
      "Epoch 3, Iteration 430, Loss: 0.05042580887675285\n",
      "Epoch 3, Iteration 440, Loss: 0.05636143684387207\n",
      "Epoch 3, Iteration 450, Loss: 0.034870099276304245\n",
      "Epoch 3, Iteration 460, Loss: 0.046026796102523804\n",
      "Epoch 3, Iteration 470, Loss: 0.03200621157884598\n",
      "Epoch 3, Iteration 480, Loss: 0.03327284753322601\n",
      "Epoch 3, Iteration 490, Loss: 0.03269697725772858\n",
      "Epoch 3, Iteration 500, Loss: 0.03924503177404404\n",
      "Epoch 3, Iteration 510, Loss: 0.05580022558569908\n",
      "Epoch 3, Iteration 520, Loss: 0.05383772403001785\n",
      "Epoch 3, Iteration 530, Loss: 0.035709839314222336\n",
      "Epoch 3, Iteration 540, Loss: 0.025561369955539703\n",
      "Epoch 3, Iteration 550, Loss: 0.05890628695487976\n",
      "Epoch 3, Iteration 560, Loss: 0.06917121261358261\n",
      "Epoch 3, Iteration 570, Loss: 0.0341692790389061\n",
      "Epoch 3, Iteration 580, Loss: 0.03060537576675415\n",
      "Epoch 3, Iteration 590, Loss: 0.028588751330971718\n",
      "Epoch 3, Iteration 600, Loss: 0.045162178575992584\n",
      "Epoch 3, Iteration 610, Loss: 0.06087968498468399\n",
      "Epoch 3, Iteration 620, Loss: 0.06495467573404312\n",
      "Epoch 3, Iteration 630, Loss: 0.025137007236480713\n",
      "Epoch 3, Iteration 640, Loss: 0.028153996914625168\n",
      "Epoch 3, Iteration 650, Loss: 0.11102966964244843\n",
      "Epoch 3, Iteration 660, Loss: 0.056396257132291794\n",
      "Epoch 3, Iteration 670, Loss: 0.03561535105109215\n",
      "Epoch 3, Iteration 680, Loss: 0.034379612654447556\n",
      "Epoch 3, Iteration 690, Loss: 0.05678384751081467\n",
      "Epoch 3, Iteration 700, Loss: 0.04281393438577652\n",
      "Epoch 3, Iteration 710, Loss: 0.04596978798508644\n",
      "Epoch 3, Iteration 720, Loss: 0.0806255042552948\n",
      "Epoch 3, Iteration 730, Loss: 0.05311688780784607\n",
      "Epoch 3, Validation Loss: 0.044327884001414415\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 4, Iteration 0, Loss: 0.032866477966308594\n",
      "Epoch 4, Iteration 10, Loss: 0.03986791521310806\n",
      "Epoch 4, Iteration 20, Loss: 0.032875895500183105\n",
      "Epoch 4, Iteration 30, Loss: 0.05354670435190201\n",
      "Epoch 4, Iteration 40, Loss: 0.039033371955156326\n",
      "Epoch 4, Iteration 50, Loss: 0.05997863784432411\n",
      "Epoch 4, Iteration 60, Loss: 0.0164056196808815\n",
      "Epoch 4, Iteration 70, Loss: 0.04685419797897339\n",
      "Epoch 4, Iteration 80, Loss: 0.0683072879910469\n",
      "Epoch 4, Iteration 90, Loss: 0.06473535299301147\n",
      "Epoch 4, Iteration 100, Loss: 0.07147250324487686\n",
      "Epoch 4, Iteration 110, Loss: 0.050376515835523605\n",
      "Epoch 4, Iteration 120, Loss: 0.03587998449802399\n",
      "Epoch 4, Iteration 130, Loss: 0.03875968977808952\n",
      "Epoch 4, Iteration 140, Loss: 0.08274462819099426\n",
      "Epoch 4, Iteration 150, Loss: 0.03304501995444298\n",
      "Epoch 4, Iteration 160, Loss: 0.021133072674274445\n",
      "Epoch 4, Iteration 170, Loss: 0.04172628000378609\n",
      "Epoch 4, Iteration 180, Loss: 0.04598874971270561\n",
      "Epoch 4, Iteration 190, Loss: 0.018059605732560158\n",
      "Epoch 4, Iteration 200, Loss: 0.052589695900678635\n",
      "Epoch 4, Iteration 210, Loss: 0.034429214894771576\n",
      "Epoch 4, Iteration 220, Loss: 0.05700810253620148\n",
      "Epoch 4, Iteration 230, Loss: 0.04601859673857689\n",
      "Epoch 4, Iteration 240, Loss: 0.052186936140060425\n",
      "Epoch 4, Iteration 250, Loss: 0.07012692093849182\n",
      "Epoch 4, Iteration 260, Loss: 0.05648020654916763\n",
      "Epoch 4, Iteration 270, Loss: 0.041263774037361145\n",
      "Epoch 4, Iteration 280, Loss: 0.04815475642681122\n",
      "Epoch 4, Iteration 290, Loss: 0.04184162616729736\n",
      "Epoch 4, Iteration 300, Loss: 0.06221233308315277\n",
      "Epoch 4, Iteration 310, Loss: 0.03128290921449661\n",
      "Epoch 4, Iteration 320, Loss: 0.03893086314201355\n",
      "Epoch 4, Iteration 330, Loss: 0.09328565746545792\n",
      "Epoch 4, Iteration 340, Loss: 0.04422749578952789\n",
      "Epoch 4, Iteration 350, Loss: 0.052402231842279434\n",
      "Epoch 4, Iteration 360, Loss: 0.04613861069083214\n",
      "Epoch 4, Iteration 370, Loss: 0.05055573582649231\n",
      "Epoch 4, Iteration 380, Loss: 0.032567400485277176\n",
      "Epoch 4, Iteration 390, Loss: 0.06352794170379639\n",
      "Epoch 4, Iteration 400, Loss: 0.031286247074604034\n",
      "Epoch 4, Iteration 410, Loss: 0.02550019696354866\n",
      "Epoch 4, Iteration 420, Loss: 0.027965227141976357\n",
      "Epoch 4, Iteration 430, Loss: 0.05600297823548317\n",
      "Epoch 4, Iteration 440, Loss: 0.05945926532149315\n",
      "Epoch 4, Iteration 450, Loss: 0.04532436281442642\n",
      "Epoch 4, Iteration 460, Loss: 0.02065504528582096\n",
      "Epoch 4, Iteration 470, Loss: 0.038980916142463684\n",
      "Epoch 4, Iteration 480, Loss: 0.06153329834342003\n",
      "Epoch 4, Iteration 490, Loss: 0.057661138474941254\n",
      "Epoch 4, Iteration 500, Loss: 0.027433529496192932\n",
      "Epoch 4, Iteration 510, Loss: 0.023041103035211563\n",
      "Epoch 4, Iteration 520, Loss: 0.016331646591424942\n",
      "Epoch 4, Iteration 530, Loss: 0.07182537764310837\n",
      "Epoch 4, Iteration 540, Loss: 0.04871545732021332\n",
      "Epoch 4, Iteration 550, Loss: 0.06295774132013321\n",
      "Epoch 4, Iteration 560, Loss: 0.045996226370334625\n",
      "Epoch 4, Iteration 570, Loss: 0.05184384807944298\n",
      "Epoch 4, Iteration 580, Loss: 0.019281337037682533\n",
      "Epoch 4, Iteration 590, Loss: 0.051407139748334885\n",
      "Epoch 4, Iteration 600, Loss: 0.06463869661092758\n",
      "Epoch 4, Iteration 610, Loss: 0.05064530670642853\n",
      "Epoch 4, Iteration 620, Loss: 0.04624640941619873\n",
      "Epoch 4, Iteration 630, Loss: 0.03395800665020943\n",
      "Epoch 4, Iteration 640, Loss: 0.04385358840227127\n",
      "Epoch 4, Iteration 650, Loss: 0.03279196843504906\n",
      "Epoch 4, Iteration 660, Loss: 0.036117564886808395\n",
      "Epoch 4, Iteration 670, Loss: 0.024966534227132797\n",
      "Epoch 4, Iteration 680, Loss: 0.03488751873373985\n",
      "Epoch 4, Iteration 690, Loss: 0.03979000076651573\n",
      "Epoch 4, Iteration 700, Loss: 0.037040259689092636\n",
      "Epoch 4, Iteration 710, Loss: 0.05921567231416702\n",
      "Epoch 4, Iteration 720, Loss: 0.05093349516391754\n",
      "Epoch 4, Iteration 730, Loss: 0.04290733113884926\n",
      "Epoch 4, Validation Loss: 0.043362695501064474\n",
      "Validation loss decreased (0.043929 --> 0.043363). Saving model...\n",
      "Epoch 5, Iteration 0, Loss: 0.029317693784832954\n",
      "Epoch 5, Iteration 10, Loss: 0.09205513447523117\n",
      "Epoch 5, Iteration 20, Loss: 0.024373594671487808\n",
      "Epoch 5, Iteration 30, Loss: 0.0754501074552536\n",
      "Epoch 5, Iteration 40, Loss: 0.032493989914655685\n",
      "Epoch 5, Iteration 50, Loss: 0.05716059356927872\n",
      "Epoch 5, Iteration 60, Loss: 0.030849836766719818\n",
      "Epoch 5, Iteration 70, Loss: 0.03154047951102257\n",
      "Epoch 5, Iteration 80, Loss: 0.04761473461985588\n",
      "Epoch 5, Iteration 90, Loss: 0.055396176874637604\n",
      "Epoch 5, Iteration 100, Loss: 0.043841056525707245\n",
      "Epoch 5, Iteration 110, Loss: 0.06116433069109917\n",
      "Epoch 5, Iteration 120, Loss: 0.03556586056947708\n",
      "Epoch 5, Iteration 130, Loss: 0.03127041831612587\n",
      "Epoch 5, Iteration 140, Loss: 0.04482102021574974\n",
      "Epoch 5, Iteration 150, Loss: 0.03321082517504692\n",
      "Epoch 5, Iteration 160, Loss: 0.04957902804017067\n",
      "Epoch 5, Iteration 170, Loss: 0.029270948842167854\n",
      "Epoch 5, Iteration 180, Loss: 0.03057246282696724\n",
      "Epoch 5, Iteration 190, Loss: 0.06520617753267288\n",
      "Epoch 5, Iteration 200, Loss: 0.04207858443260193\n",
      "Epoch 5, Iteration 210, Loss: 0.06110450252890587\n",
      "Epoch 5, Iteration 220, Loss: 0.029307939112186432\n",
      "Epoch 5, Iteration 230, Loss: 0.04847605898976326\n",
      "Epoch 5, Iteration 240, Loss: 0.03290225565433502\n",
      "Epoch 5, Iteration 250, Loss: 0.019568972289562225\n",
      "Epoch 5, Iteration 260, Loss: 0.05767768248915672\n",
      "Epoch 5, Iteration 270, Loss: 0.023303436115384102\n",
      "Epoch 5, Iteration 280, Loss: 0.07923265546560287\n",
      "Epoch 5, Iteration 290, Loss: 0.05837458372116089\n",
      "Epoch 5, Iteration 300, Loss: 0.035971831530332565\n",
      "Epoch 5, Iteration 310, Loss: 0.0824674740433693\n",
      "Epoch 5, Iteration 320, Loss: 0.03738798573613167\n",
      "Epoch 5, Iteration 330, Loss: 0.035905662924051285\n",
      "Epoch 5, Iteration 340, Loss: 0.07935880869626999\n",
      "Epoch 5, Iteration 350, Loss: 0.025276552885770798\n",
      "Epoch 5, Iteration 360, Loss: 0.026275282725691795\n",
      "Epoch 5, Iteration 370, Loss: 0.04757925122976303\n",
      "Epoch 5, Iteration 380, Loss: 0.034096166491508484\n",
      "Epoch 5, Iteration 390, Loss: 0.08263798803091049\n",
      "Epoch 5, Iteration 400, Loss: 0.025319159030914307\n",
      "Epoch 5, Iteration 410, Loss: 0.0437893383204937\n",
      "Epoch 5, Iteration 420, Loss: 0.05603925138711929\n",
      "Epoch 5, Iteration 430, Loss: 0.04597514495253563\n",
      "Epoch 5, Iteration 440, Loss: 0.030890025198459625\n",
      "Epoch 5, Iteration 450, Loss: 0.05833795666694641\n",
      "Epoch 5, Iteration 460, Loss: 0.03315385803580284\n",
      "Epoch 5, Iteration 470, Loss: 0.052106063812971115\n",
      "Epoch 5, Iteration 480, Loss: 0.05259484052658081\n",
      "Epoch 5, Iteration 490, Loss: 0.03152812272310257\n",
      "Epoch 5, Iteration 500, Loss: 0.06213245540857315\n",
      "Epoch 5, Iteration 510, Loss: 0.05464043468236923\n",
      "Epoch 5, Iteration 520, Loss: 0.07124457508325577\n",
      "Epoch 5, Iteration 530, Loss: 0.06034999340772629\n",
      "Epoch 5, Iteration 540, Loss: 0.036017194390296936\n",
      "Epoch 5, Iteration 550, Loss: 0.027495726943016052\n",
      "Epoch 5, Iteration 560, Loss: 0.030590025708079338\n",
      "Epoch 5, Iteration 570, Loss: 0.039129551500082016\n",
      "Epoch 5, Iteration 580, Loss: 0.03969182074069977\n",
      "Epoch 5, Iteration 590, Loss: 0.030198754742741585\n",
      "Epoch 5, Iteration 600, Loss: 0.032576143741607666\n",
      "Epoch 5, Iteration 610, Loss: 0.04909788817167282\n",
      "Epoch 5, Iteration 620, Loss: 0.03176293894648552\n",
      "Epoch 5, Iteration 630, Loss: 0.03596346452832222\n",
      "Epoch 5, Iteration 640, Loss: 0.03452060744166374\n",
      "Epoch 5, Iteration 650, Loss: 0.053720444440841675\n",
      "Epoch 5, Iteration 660, Loss: 0.05164753273129463\n",
      "Epoch 5, Iteration 670, Loss: 0.07011138647794724\n",
      "Epoch 5, Iteration 680, Loss: 0.08662842959165573\n",
      "Epoch 5, Iteration 690, Loss: 0.028694484382867813\n",
      "Epoch 5, Iteration 700, Loss: 0.045895423740148544\n",
      "Epoch 5, Iteration 710, Loss: 0.0525592640042305\n",
      "Epoch 5, Iteration 720, Loss: 0.036730535328388214\n",
      "Epoch 5, Iteration 730, Loss: 0.053938109427690506\n",
      "Epoch 5, Validation Loss: 0.04360044656483376\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 6, Iteration 0, Loss: 0.07101792842149734\n",
      "Epoch 6, Iteration 10, Loss: 0.06604985147714615\n",
      "Epoch 6, Iteration 20, Loss: 0.059551384299993515\n",
      "Epoch 6, Iteration 30, Loss: 0.03839860111474991\n",
      "Epoch 6, Iteration 40, Loss: 0.03566443547606468\n",
      "Epoch 6, Iteration 50, Loss: 0.08038152009248734\n",
      "Epoch 6, Iteration 60, Loss: 0.05094136670231819\n",
      "Epoch 6, Iteration 70, Loss: 0.03642567992210388\n",
      "Epoch 6, Iteration 80, Loss: 0.024442115798592567\n",
      "Epoch 6, Iteration 90, Loss: 0.032281793653964996\n",
      "Epoch 6, Iteration 100, Loss: 0.02702604979276657\n",
      "Epoch 6, Iteration 110, Loss: 0.030991649255156517\n",
      "Epoch 6, Iteration 120, Loss: 0.03547556325793266\n",
      "Epoch 6, Iteration 130, Loss: 0.07429726421833038\n",
      "Epoch 6, Iteration 140, Loss: 0.03865605965256691\n",
      "Epoch 6, Iteration 150, Loss: 0.08346188813447952\n",
      "Epoch 6, Iteration 160, Loss: 0.05655619874596596\n",
      "Epoch 6, Iteration 170, Loss: 0.04133168235421181\n",
      "Epoch 6, Iteration 180, Loss: 0.08093946427106857\n",
      "Epoch 6, Iteration 190, Loss: 0.04013156145811081\n",
      "Epoch 6, Iteration 200, Loss: 0.08155519515275955\n",
      "Epoch 6, Iteration 210, Loss: 0.04832387715578079\n",
      "Epoch 6, Iteration 220, Loss: 0.020084084942936897\n",
      "Epoch 6, Iteration 230, Loss: 0.03585429489612579\n",
      "Epoch 6, Iteration 240, Loss: 0.09378866106271744\n",
      "Epoch 6, Iteration 250, Loss: 0.07961245626211166\n",
      "Epoch 6, Iteration 260, Loss: 0.04319937527179718\n",
      "Epoch 6, Iteration 270, Loss: 0.04971666634082794\n",
      "Epoch 6, Iteration 280, Loss: 0.0408027358353138\n",
      "Epoch 6, Iteration 290, Loss: 0.09132414311170578\n",
      "Epoch 6, Iteration 300, Loss: 0.02290218509733677\n",
      "Epoch 6, Iteration 310, Loss: 0.0856533795595169\n",
      "Epoch 6, Iteration 320, Loss: 0.04052522033452988\n",
      "Epoch 6, Iteration 330, Loss: 0.01748015731573105\n",
      "Epoch 6, Iteration 340, Loss: 0.07755201309919357\n",
      "Epoch 6, Iteration 350, Loss: 0.043312717229127884\n",
      "Epoch 6, Iteration 360, Loss: 0.041014134883880615\n",
      "Epoch 6, Iteration 370, Loss: 0.03917727991938591\n",
      "Epoch 6, Iteration 380, Loss: 0.046368543058633804\n",
      "Epoch 6, Iteration 390, Loss: 0.03309161961078644\n",
      "Epoch 6, Iteration 400, Loss: 0.06673474609851837\n",
      "Epoch 6, Iteration 410, Loss: 0.052314642816782\n",
      "Epoch 6, Iteration 420, Loss: 0.05583080276846886\n",
      "Epoch 6, Iteration 430, Loss: 0.0300454031676054\n",
      "Epoch 6, Iteration 440, Loss: 0.038624491542577744\n",
      "Epoch 6, Iteration 450, Loss: 0.03212868049740791\n",
      "Epoch 6, Iteration 460, Loss: 0.026051677763462067\n",
      "Epoch 6, Iteration 470, Loss: 0.05104050785303116\n",
      "Epoch 6, Iteration 480, Loss: 0.045177437365055084\n",
      "Epoch 6, Iteration 490, Loss: 0.039929892867803574\n",
      "Epoch 6, Iteration 500, Loss: 0.047391727566719055\n",
      "Epoch 6, Iteration 510, Loss: 0.06533006578683853\n",
      "Epoch 6, Iteration 520, Loss: 0.07363897562026978\n",
      "Epoch 6, Iteration 530, Loss: 0.04075418785214424\n",
      "Epoch 6, Iteration 540, Loss: 0.03493158891797066\n",
      "Epoch 6, Iteration 550, Loss: 0.01978156343102455\n",
      "Epoch 6, Iteration 560, Loss: 0.03667384758591652\n",
      "Epoch 6, Iteration 570, Loss: 0.043151043355464935\n",
      "Epoch 6, Iteration 580, Loss: 0.04682537168264389\n",
      "Epoch 6, Iteration 590, Loss: 0.07266155630350113\n",
      "Epoch 6, Iteration 600, Loss: 0.04763166606426239\n",
      "Epoch 6, Iteration 610, Loss: 0.05331478267908096\n",
      "Epoch 6, Iteration 620, Loss: 0.029297692701220512\n",
      "Epoch 6, Iteration 630, Loss: 0.039923105388879776\n",
      "Epoch 6, Iteration 640, Loss: 0.02767859399318695\n",
      "Epoch 6, Iteration 650, Loss: 0.04801136255264282\n",
      "Epoch 6, Iteration 660, Loss: 0.04050084576010704\n",
      "Epoch 6, Iteration 670, Loss: 0.0514674074947834\n",
      "Epoch 6, Iteration 680, Loss: 0.02465014159679413\n",
      "Epoch 6, Iteration 690, Loss: 0.06195065751671791\n",
      "Epoch 6, Iteration 700, Loss: 0.04671379551291466\n",
      "Epoch 6, Iteration 710, Loss: 0.03909830003976822\n",
      "Epoch 6, Iteration 720, Loss: 0.03917119279503822\n",
      "Epoch 6, Iteration 730, Loss: 0.024721862748265266\n",
      "Epoch 6, Validation Loss: 0.04344849934315552\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 7, Iteration 0, Loss: 0.04276007413864136\n",
      "Epoch 7, Iteration 10, Loss: 0.028103401884436607\n",
      "Epoch 7, Iteration 20, Loss: 0.04898711293935776\n",
      "Epoch 7, Iteration 30, Loss: 0.04351727291941643\n",
      "Epoch 7, Iteration 40, Loss: 0.02774953469634056\n",
      "Epoch 7, Iteration 50, Loss: 0.06579524278640747\n",
      "Epoch 7, Iteration 60, Loss: 0.03890654072165489\n",
      "Epoch 7, Iteration 70, Loss: 0.04729404300451279\n",
      "Epoch 7, Iteration 80, Loss: 0.052596550434827805\n",
      "Epoch 7, Iteration 90, Loss: 0.025785446166992188\n",
      "Epoch 7, Iteration 100, Loss: 0.04115957021713257\n",
      "Epoch 7, Iteration 110, Loss: 0.036404889076948166\n",
      "Epoch 7, Iteration 120, Loss: 0.0555352158844471\n",
      "Epoch 7, Iteration 130, Loss: 0.04624870419502258\n",
      "Epoch 7, Iteration 140, Loss: 0.017942208796739578\n",
      "Epoch 7, Iteration 150, Loss: 0.10924620181322098\n",
      "Epoch 7, Iteration 160, Loss: 0.02996385470032692\n",
      "Epoch 7, Iteration 170, Loss: 0.061673637479543686\n",
      "Epoch 7, Iteration 180, Loss: 0.025079289451241493\n",
      "Epoch 7, Iteration 190, Loss: 0.03419491648674011\n",
      "Epoch 7, Iteration 200, Loss: 0.058349717408418655\n",
      "Epoch 7, Iteration 210, Loss: 0.03689546510577202\n",
      "Epoch 7, Iteration 220, Loss: 0.03722178936004639\n",
      "Epoch 7, Iteration 230, Loss: 0.0511249378323555\n",
      "Epoch 7, Iteration 240, Loss: 0.06299338489770889\n",
      "Epoch 7, Iteration 250, Loss: 0.03352290391921997\n",
      "Epoch 7, Iteration 260, Loss: 0.0285020861774683\n",
      "Epoch 7, Iteration 270, Loss: 0.04347340762615204\n",
      "Epoch 7, Iteration 280, Loss: 0.047866079956293106\n",
      "Epoch 7, Iteration 290, Loss: 0.051827386021614075\n",
      "Epoch 7, Iteration 300, Loss: 0.046152859926223755\n",
      "Epoch 7, Iteration 310, Loss: 0.02071048505604267\n",
      "Epoch 7, Iteration 320, Loss: 0.03000437282025814\n",
      "Epoch 7, Iteration 330, Loss: 0.040321607142686844\n",
      "Epoch 7, Iteration 340, Loss: 0.04404381290078163\n",
      "Epoch 7, Iteration 350, Loss: 0.049163635820150375\n",
      "Epoch 7, Iteration 360, Loss: 0.04986603185534477\n",
      "Epoch 7, Iteration 370, Loss: 0.04573255032300949\n",
      "Epoch 7, Iteration 380, Loss: 0.05207551270723343\n",
      "Epoch 7, Iteration 390, Loss: 0.030100425705313683\n",
      "Epoch 7, Iteration 400, Loss: 0.027151046320796013\n",
      "Epoch 7, Iteration 410, Loss: 0.05485496670007706\n",
      "Epoch 7, Iteration 420, Loss: 0.06252116709947586\n",
      "Epoch 7, Iteration 430, Loss: 0.053369201719760895\n",
      "Epoch 7, Iteration 440, Loss: 0.049659617245197296\n",
      "Epoch 7, Iteration 450, Loss: 0.03619828447699547\n",
      "Epoch 7, Iteration 460, Loss: 0.026837456971406937\n",
      "Epoch 7, Iteration 470, Loss: 0.047288306057453156\n",
      "Epoch 7, Iteration 480, Loss: 0.028962962329387665\n",
      "Epoch 7, Iteration 490, Loss: 0.03589165583252907\n",
      "Epoch 7, Iteration 500, Loss: 0.03163490816950798\n",
      "Epoch 7, Iteration 510, Loss: 0.03654737398028374\n",
      "Epoch 7, Iteration 520, Loss: 0.050982747226953506\n",
      "Epoch 7, Iteration 530, Loss: 0.04429106041789055\n",
      "Epoch 7, Iteration 540, Loss: 0.04383966699242592\n",
      "Epoch 7, Iteration 550, Loss: 0.03835713863372803\n",
      "Epoch 7, Iteration 560, Loss: 0.053227055817842484\n",
      "Epoch 7, Iteration 570, Loss: 0.03452802821993828\n",
      "Epoch 7, Iteration 580, Loss: 0.028581729158759117\n",
      "Epoch 7, Iteration 590, Loss: 0.03209049254655838\n",
      "Epoch 7, Iteration 600, Loss: 0.042389385402202606\n",
      "Epoch 7, Iteration 610, Loss: 0.061709336936473846\n",
      "Epoch 7, Iteration 620, Loss: 0.048491522669792175\n",
      "Epoch 7, Iteration 630, Loss: 0.05652265623211861\n",
      "Epoch 7, Iteration 640, Loss: 0.11305472999811172\n",
      "Epoch 7, Iteration 650, Loss: 0.04770988970994949\n",
      "Epoch 7, Iteration 660, Loss: 0.03215482085943222\n",
      "Epoch 7, Iteration 670, Loss: 0.048878028988838196\n",
      "Epoch 7, Iteration 680, Loss: 0.0555468313395977\n",
      "Epoch 7, Iteration 690, Loss: 0.051114749163389206\n",
      "Epoch 7, Iteration 700, Loss: 0.07333027571439743\n",
      "Epoch 7, Iteration 710, Loss: 0.04450826346874237\n",
      "Epoch 7, Iteration 720, Loss: 0.10172643512487411\n",
      "Epoch 7, Iteration 730, Loss: 0.038125891238451004\n",
      "Epoch 7, Validation Loss: 0.042787975874607975\n",
      "Validation loss decreased (0.043363 --> 0.042788). Saving model...\n",
      "Epoch 8, Iteration 0, Loss: 0.05077339708805084\n",
      "Epoch 8, Iteration 10, Loss: 0.033131200820207596\n",
      "Epoch 8, Iteration 20, Loss: 0.07815451174974442\n",
      "Epoch 8, Iteration 30, Loss: 0.023585274815559387\n",
      "Epoch 8, Iteration 40, Loss: 0.05275312438607216\n",
      "Epoch 8, Iteration 50, Loss: 0.05362750589847565\n",
      "Epoch 8, Iteration 60, Loss: 0.08966459333896637\n",
      "Epoch 8, Iteration 70, Loss: 0.03857836872339249\n",
      "Epoch 8, Iteration 80, Loss: 0.04716159775853157\n",
      "Epoch 8, Iteration 90, Loss: 0.020771458745002747\n",
      "Epoch 8, Iteration 100, Loss: 0.04022790491580963\n",
      "Epoch 8, Iteration 110, Loss: 0.030557353049516678\n",
      "Epoch 8, Iteration 120, Loss: 0.038626447319984436\n",
      "Epoch 8, Iteration 130, Loss: 0.04520602151751518\n",
      "Epoch 8, Iteration 140, Loss: 0.041868966072797775\n",
      "Epoch 8, Iteration 150, Loss: 0.03962158039212227\n",
      "Epoch 8, Iteration 160, Loss: 0.04074814170598984\n",
      "Epoch 8, Iteration 170, Loss: 0.04619074985384941\n",
      "Epoch 8, Iteration 180, Loss: 0.04238564893603325\n",
      "Epoch 8, Iteration 190, Loss: 0.05769532173871994\n",
      "Epoch 8, Iteration 200, Loss: 0.052177347242832184\n",
      "Epoch 8, Iteration 210, Loss: 0.033874742686748505\n",
      "Epoch 8, Iteration 220, Loss: 0.0351094976067543\n",
      "Epoch 8, Iteration 230, Loss: 0.018033232539892197\n",
      "Epoch 8, Iteration 240, Loss: 0.05315878614783287\n",
      "Epoch 8, Iteration 250, Loss: 0.03343459218740463\n",
      "Epoch 8, Iteration 260, Loss: 0.034890998154878616\n",
      "Epoch 8, Iteration 270, Loss: 0.0493677482008934\n",
      "Epoch 8, Iteration 280, Loss: 0.045169685035943985\n",
      "Epoch 8, Iteration 290, Loss: 0.05472357198596001\n",
      "Epoch 8, Iteration 300, Loss: 0.047703441232442856\n",
      "Epoch 8, Iteration 310, Loss: 0.05213746055960655\n",
      "Epoch 8, Iteration 320, Loss: 0.10320203751325607\n",
      "Epoch 8, Iteration 330, Loss: 0.049883097410202026\n",
      "Epoch 8, Iteration 340, Loss: 0.029432330280542374\n",
      "Epoch 8, Iteration 350, Loss: 0.04645582661032677\n",
      "Epoch 8, Iteration 360, Loss: 0.052140362560749054\n",
      "Epoch 8, Iteration 370, Loss: 0.012507753446698189\n",
      "Epoch 8, Iteration 380, Loss: 0.08818698674440384\n",
      "Epoch 8, Iteration 390, Loss: 0.03651771321892738\n",
      "Epoch 8, Iteration 400, Loss: 0.03461573272943497\n",
      "Epoch 8, Iteration 410, Loss: 0.02167254313826561\n",
      "Epoch 8, Iteration 420, Loss: 0.0388365313410759\n",
      "Epoch 8, Iteration 430, Loss: 0.12112502008676529\n",
      "Epoch 8, Iteration 440, Loss: 0.03490923345088959\n",
      "Epoch 8, Iteration 450, Loss: 0.059250425547361374\n",
      "Epoch 8, Iteration 460, Loss: 0.058137472718954086\n",
      "Epoch 8, Iteration 470, Loss: 0.03497391194105148\n",
      "Epoch 8, Iteration 480, Loss: 0.04325041547417641\n",
      "Epoch 8, Iteration 490, Loss: 0.11100540310144424\n",
      "Epoch 8, Iteration 500, Loss: 0.04738282784819603\n",
      "Epoch 8, Iteration 510, Loss: 0.03388004004955292\n",
      "Epoch 8, Iteration 520, Loss: 0.03768685832619667\n",
      "Epoch 8, Iteration 530, Loss: 0.04368499666452408\n",
      "Epoch 8, Iteration 540, Loss: 0.034084778279066086\n",
      "Epoch 8, Iteration 550, Loss: 0.01730198599398136\n",
      "Epoch 8, Iteration 560, Loss: 0.04371185973286629\n",
      "Epoch 8, Iteration 570, Loss: 0.0681033581495285\n",
      "Epoch 8, Iteration 580, Loss: 0.03655494749546051\n",
      "Epoch 8, Iteration 590, Loss: 0.03832888603210449\n",
      "Epoch 8, Iteration 600, Loss: 0.06263755261898041\n",
      "Epoch 8, Iteration 610, Loss: 0.0290274228900671\n",
      "Epoch 8, Iteration 620, Loss: 0.034607984125614166\n",
      "Epoch 8, Iteration 630, Loss: 0.017307426780462265\n",
      "Epoch 8, Iteration 640, Loss: 0.0358729250729084\n",
      "Epoch 8, Iteration 650, Loss: 0.03757590427994728\n",
      "Epoch 8, Iteration 660, Loss: 0.02273828722536564\n",
      "Epoch 8, Iteration 670, Loss: 0.036027152091264725\n",
      "Epoch 8, Iteration 680, Loss: 0.022794265300035477\n",
      "Epoch 8, Iteration 690, Loss: 0.078467957675457\n",
      "Epoch 8, Iteration 700, Loss: 0.04547657072544098\n",
      "Epoch 8, Iteration 710, Loss: 0.0183243565261364\n",
      "Epoch 8, Iteration 720, Loss: 0.034282613545656204\n",
      "Epoch 8, Iteration 730, Loss: 0.041392624378204346\n",
      "Epoch 8, Validation Loss: 0.04282259445070573\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 9, Iteration 0, Loss: 0.06413458287715912\n",
      "Epoch 9, Iteration 10, Loss: 0.025946075096726418\n",
      "Epoch 9, Iteration 20, Loss: 0.04742438718676567\n",
      "Epoch 9, Iteration 30, Loss: 0.03241857886314392\n",
      "Epoch 9, Iteration 40, Loss: 0.025875579565763474\n",
      "Epoch 9, Iteration 50, Loss: 0.0539216548204422\n",
      "Epoch 9, Iteration 60, Loss: 0.052664615213871\n",
      "Epoch 9, Iteration 70, Loss: 0.04360289126634598\n",
      "Epoch 9, Iteration 80, Loss: 0.020823657512664795\n",
      "Epoch 9, Iteration 90, Loss: 0.042369358241558075\n",
      "Epoch 9, Iteration 100, Loss: 0.04291725158691406\n",
      "Epoch 9, Iteration 110, Loss: 0.026135697960853577\n",
      "Epoch 9, Iteration 120, Loss: 0.09259043633937836\n",
      "Epoch 9, Iteration 130, Loss: 0.04472249746322632\n",
      "Epoch 9, Iteration 140, Loss: 0.051743026822805405\n",
      "Epoch 9, Iteration 150, Loss: 0.05179087445139885\n",
      "Epoch 9, Iteration 160, Loss: 0.032621562480926514\n",
      "Epoch 9, Iteration 170, Loss: 0.022907961159944534\n",
      "Epoch 9, Iteration 180, Loss: 0.031881604343652725\n",
      "Epoch 9, Iteration 190, Loss: 0.06772571802139282\n",
      "Epoch 9, Iteration 200, Loss: 0.0370221845805645\n",
      "Epoch 9, Iteration 210, Loss: 0.07079887390136719\n",
      "Epoch 9, Iteration 220, Loss: 0.08882684260606766\n",
      "Epoch 9, Iteration 230, Loss: 0.03728092461824417\n",
      "Epoch 9, Iteration 240, Loss: 0.0366930291056633\n",
      "Epoch 9, Iteration 250, Loss: 0.05709826946258545\n",
      "Epoch 9, Iteration 260, Loss: 0.04023530334234238\n",
      "Epoch 9, Iteration 270, Loss: 0.031304311007261276\n",
      "Epoch 9, Iteration 280, Loss: 0.044993095099925995\n",
      "Epoch 9, Iteration 290, Loss: 0.024832833558321\n",
      "Epoch 9, Iteration 300, Loss: 0.05759670212864876\n",
      "Epoch 9, Iteration 310, Loss: 0.02798292227089405\n",
      "Epoch 9, Iteration 320, Loss: 0.04562004283070564\n",
      "Epoch 9, Iteration 330, Loss: 0.1037282943725586\n",
      "Epoch 9, Iteration 340, Loss: 0.026987364515662193\n",
      "Epoch 9, Iteration 350, Loss: 0.05554686114192009\n",
      "Epoch 9, Iteration 360, Loss: 0.03494872897863388\n",
      "Epoch 9, Iteration 370, Loss: 0.025604087859392166\n",
      "Epoch 9, Iteration 380, Loss: 0.02154397778213024\n",
      "Epoch 9, Iteration 390, Loss: 0.052857644855976105\n",
      "Epoch 9, Iteration 400, Loss: 0.03693227842450142\n",
      "Epoch 9, Iteration 410, Loss: 0.041777268052101135\n",
      "Epoch 9, Iteration 420, Loss: 0.04639916121959686\n",
      "Epoch 9, Iteration 430, Loss: 0.02011488378047943\n",
      "Epoch 9, Iteration 440, Loss: 0.05122293531894684\n",
      "Epoch 9, Iteration 450, Loss: 0.03640042617917061\n",
      "Epoch 9, Iteration 460, Loss: 0.020614610984921455\n",
      "Epoch 9, Iteration 470, Loss: 0.024266252294182777\n",
      "Epoch 9, Iteration 480, Loss: 0.02652105875313282\n",
      "Epoch 9, Iteration 490, Loss: 0.03232164680957794\n",
      "Epoch 9, Iteration 500, Loss: 0.0684540867805481\n",
      "Epoch 9, Iteration 510, Loss: 0.05832592770457268\n",
      "Epoch 9, Iteration 520, Loss: 0.08605527132749557\n",
      "Epoch 9, Iteration 530, Loss: 0.02916385419666767\n",
      "Epoch 9, Iteration 540, Loss: 0.06842659413814545\n",
      "Epoch 9, Iteration 550, Loss: 0.028579607605934143\n",
      "Epoch 9, Iteration 560, Loss: 0.03590569272637367\n",
      "Epoch 9, Iteration 570, Loss: 0.05781978741288185\n",
      "Epoch 9, Iteration 580, Loss: 0.032065391540527344\n",
      "Epoch 9, Iteration 590, Loss: 0.02792426012456417\n",
      "Epoch 9, Iteration 600, Loss: 0.07143614441156387\n",
      "Epoch 9, Iteration 610, Loss: 0.03465999662876129\n",
      "Epoch 9, Iteration 620, Loss: 0.04581540822982788\n",
      "Epoch 9, Iteration 630, Loss: 0.03529536724090576\n",
      "Epoch 9, Iteration 640, Loss: 0.035009291023015976\n",
      "Epoch 9, Iteration 650, Loss: 0.035227563232183456\n",
      "Epoch 9, Iteration 660, Loss: 0.09574352204799652\n",
      "Epoch 9, Iteration 670, Loss: 0.033826813101768494\n",
      "Epoch 9, Iteration 680, Loss: 0.01629580557346344\n",
      "Epoch 9, Iteration 690, Loss: 0.030403990298509598\n",
      "Epoch 9, Iteration 700, Loss: 0.06443566083908081\n",
      "Epoch 9, Iteration 710, Loss: 0.06594860553741455\n",
      "Epoch 9, Iteration 720, Loss: 0.0396786704659462\n",
      "Epoch 9, Iteration 730, Loss: 0.04722665622830391\n",
      "Epoch 9, Validation Loss: 0.04260886420050393\n",
      "Validation loss decreased (0.042788 --> 0.042609). Saving model...\n",
      "Epoch 10, Iteration 0, Loss: 0.056884292513132095\n",
      "Epoch 10, Iteration 10, Loss: 0.023428065702319145\n",
      "Epoch 10, Iteration 20, Loss: 0.024367518723011017\n",
      "Epoch 10, Iteration 30, Loss: 0.04950952157378197\n",
      "Epoch 10, Iteration 40, Loss: 0.03687603399157524\n",
      "Epoch 10, Iteration 50, Loss: 0.030970506370067596\n",
      "Epoch 10, Iteration 60, Loss: 0.07038065791130066\n",
      "Epoch 10, Iteration 70, Loss: 0.06468504667282104\n",
      "Epoch 10, Iteration 80, Loss: 0.0616130493581295\n",
      "Epoch 10, Iteration 90, Loss: 0.0377158559858799\n",
      "Epoch 10, Iteration 100, Loss: 0.03240472450852394\n",
      "Epoch 10, Iteration 110, Loss: 0.0263836607336998\n",
      "Epoch 10, Iteration 120, Loss: 0.03197687864303589\n",
      "Epoch 10, Iteration 130, Loss: 0.07610395550727844\n",
      "Epoch 10, Iteration 140, Loss: 0.048112064599990845\n",
      "Epoch 10, Iteration 150, Loss: 0.03136745095252991\n",
      "Epoch 10, Iteration 160, Loss: 0.04856831952929497\n",
      "Epoch 10, Iteration 170, Loss: 0.04072319343686104\n",
      "Epoch 10, Iteration 180, Loss: 0.061399877071380615\n",
      "Epoch 10, Iteration 190, Loss: 0.05037296935915947\n",
      "Epoch 10, Iteration 200, Loss: 0.05326960235834122\n",
      "Epoch 10, Iteration 210, Loss: 0.014573310501873493\n",
      "Epoch 10, Iteration 220, Loss: 0.04714331030845642\n",
      "Epoch 10, Iteration 230, Loss: 0.022874774411320686\n",
      "Epoch 10, Iteration 240, Loss: 0.014114992693066597\n",
      "Epoch 10, Iteration 250, Loss: 0.03713444247841835\n",
      "Epoch 10, Iteration 260, Loss: 0.08348976075649261\n",
      "Epoch 10, Iteration 270, Loss: 0.033697038888931274\n",
      "Epoch 10, Iteration 280, Loss: 0.03822094202041626\n",
      "Epoch 10, Iteration 290, Loss: 0.030272401869297028\n",
      "Epoch 10, Iteration 300, Loss: 0.02623399905860424\n",
      "Epoch 10, Iteration 310, Loss: 0.057533133774995804\n",
      "Epoch 10, Iteration 320, Loss: 0.040731050074100494\n",
      "Epoch 10, Iteration 330, Loss: 0.031346965581178665\n",
      "Epoch 10, Iteration 340, Loss: 0.06024296209216118\n",
      "Epoch 10, Iteration 350, Loss: 0.04270361363887787\n",
      "Epoch 10, Iteration 360, Loss: 0.03655468672513962\n",
      "Epoch 10, Iteration 370, Loss: 0.05419474467635155\n",
      "Epoch 10, Iteration 380, Loss: 0.05925455316901207\n",
      "Epoch 10, Iteration 390, Loss: 0.06056680902838707\n",
      "Epoch 10, Iteration 400, Loss: 0.062088821083307266\n",
      "Epoch 10, Iteration 410, Loss: 0.10139605402946472\n",
      "Epoch 10, Iteration 420, Loss: 0.02795732393860817\n",
      "Epoch 10, Iteration 430, Loss: 0.04361698031425476\n",
      "Epoch 10, Iteration 440, Loss: 0.03568156063556671\n",
      "Epoch 10, Iteration 450, Loss: 0.06118563190102577\n",
      "Epoch 10, Iteration 460, Loss: 0.021800195798277855\n",
      "Epoch 10, Iteration 470, Loss: 0.025429217144846916\n",
      "Epoch 10, Iteration 480, Loss: 0.035519737750291824\n",
      "Epoch 10, Iteration 490, Loss: 0.058574408292770386\n",
      "Epoch 10, Iteration 500, Loss: 0.04694110155105591\n",
      "Epoch 10, Iteration 510, Loss: 0.03327695652842522\n",
      "Epoch 10, Iteration 520, Loss: 0.05713852867484093\n",
      "Epoch 10, Iteration 530, Loss: 0.051148515194654465\n",
      "Epoch 10, Iteration 540, Loss: 0.04151057451963425\n",
      "Epoch 10, Iteration 550, Loss: 0.029413273558020592\n",
      "Epoch 10, Iteration 560, Loss: 0.03957189992070198\n",
      "Epoch 10, Iteration 570, Loss: 0.04312366247177124\n",
      "Epoch 10, Iteration 580, Loss: 0.03421653434634209\n",
      "Epoch 10, Iteration 590, Loss: 0.01588430069386959\n",
      "Epoch 10, Iteration 600, Loss: 0.03936252370476723\n",
      "Epoch 10, Iteration 610, Loss: 0.025979170575737953\n",
      "Epoch 10, Iteration 620, Loss: 0.02393050491809845\n",
      "Epoch 10, Iteration 630, Loss: 0.020459579303860664\n",
      "Epoch 10, Iteration 640, Loss: 0.059364233165979385\n",
      "Epoch 10, Iteration 650, Loss: 0.026972075924277306\n",
      "Epoch 10, Iteration 660, Loss: 0.012402952648699284\n",
      "Epoch 10, Iteration 670, Loss: 0.04273802787065506\n",
      "Epoch 10, Iteration 680, Loss: 0.07440875470638275\n",
      "Epoch 10, Iteration 690, Loss: 0.045592326670885086\n",
      "Epoch 10, Iteration 700, Loss: 0.037299733608961105\n",
      "Epoch 10, Iteration 710, Loss: 0.04775882139801979\n",
      "Epoch 10, Iteration 720, Loss: 0.05722222849726677\n",
      "Epoch 10, Iteration 730, Loss: 0.05345803126692772\n",
      "Epoch 10, Validation Loss: 0.042378562522809145\n",
      "Validation loss decreased (0.042609 --> 0.042379). Saving model...\n",
      "Epoch 11, Iteration 0, Loss: 0.06554100662469864\n",
      "Epoch 11, Iteration 10, Loss: 0.05586840584874153\n",
      "Epoch 11, Iteration 20, Loss: 0.0492631196975708\n",
      "Epoch 11, Iteration 30, Loss: 0.06297830492258072\n",
      "Epoch 11, Iteration 40, Loss: 0.04310925677418709\n",
      "Epoch 11, Iteration 50, Loss: 0.04384405538439751\n",
      "Epoch 11, Iteration 60, Loss: 0.02616116590797901\n",
      "Epoch 11, Iteration 70, Loss: 0.049759216606616974\n",
      "Epoch 11, Iteration 80, Loss: 0.07444635778665543\n",
      "Epoch 11, Iteration 90, Loss: 0.03305181488394737\n",
      "Epoch 11, Iteration 100, Loss: 0.03057093359529972\n",
      "Epoch 11, Iteration 110, Loss: 0.03667101636528969\n",
      "Epoch 11, Iteration 120, Loss: 0.04526885226368904\n",
      "Epoch 11, Iteration 130, Loss: 0.050410233438014984\n",
      "Epoch 11, Iteration 140, Loss: 0.03251395747065544\n",
      "Epoch 11, Iteration 150, Loss: 0.02499566785991192\n",
      "Epoch 11, Iteration 160, Loss: 0.028144994750618935\n",
      "Epoch 11, Iteration 170, Loss: 0.060357194393873215\n",
      "Epoch 11, Iteration 180, Loss: 0.02619328163564205\n",
      "Epoch 11, Iteration 190, Loss: 0.04385243356227875\n",
      "Epoch 11, Iteration 200, Loss: 0.052792783826589584\n",
      "Epoch 11, Iteration 210, Loss: 0.06863109022378922\n",
      "Epoch 11, Iteration 220, Loss: 0.02209382690489292\n",
      "Epoch 11, Iteration 230, Loss: 0.028162578120827675\n",
      "Epoch 11, Iteration 240, Loss: 0.03181234747171402\n",
      "Epoch 11, Iteration 250, Loss: 0.02654205821454525\n",
      "Epoch 11, Iteration 260, Loss: 0.04729068651795387\n",
      "Epoch 11, Iteration 270, Loss: 0.02120182104408741\n",
      "Epoch 11, Iteration 280, Loss: 0.07036072760820389\n",
      "Epoch 11, Iteration 290, Loss: 0.028793588280677795\n",
      "Epoch 11, Iteration 300, Loss: 0.03308213874697685\n",
      "Epoch 11, Iteration 310, Loss: 0.04326782748103142\n",
      "Epoch 11, Iteration 320, Loss: 0.023866701871156693\n",
      "Epoch 11, Iteration 330, Loss: 0.05619655176997185\n",
      "Epoch 11, Iteration 340, Loss: 0.03481777384877205\n",
      "Epoch 11, Iteration 350, Loss: 0.0520595982670784\n",
      "Epoch 11, Iteration 360, Loss: 0.027481334283947945\n",
      "Epoch 11, Iteration 370, Loss: 0.037116069346666336\n",
      "Epoch 11, Iteration 380, Loss: 0.031470298767089844\n",
      "Epoch 11, Iteration 390, Loss: 0.03632183000445366\n",
      "Epoch 11, Iteration 400, Loss: 0.02552511915564537\n",
      "Epoch 11, Iteration 410, Loss: 0.021140623837709427\n",
      "Epoch 11, Iteration 420, Loss: 0.02119453437626362\n",
      "Epoch 11, Iteration 430, Loss: 0.06612323969602585\n",
      "Epoch 11, Iteration 440, Loss: 0.029571590945124626\n",
      "Epoch 11, Iteration 450, Loss: 0.02916460484266281\n",
      "Epoch 11, Iteration 460, Loss: 0.06651514768600464\n",
      "Epoch 11, Iteration 470, Loss: 0.047607358545064926\n",
      "Epoch 11, Iteration 480, Loss: 0.05144009739160538\n",
      "Epoch 11, Iteration 490, Loss: 0.043682508170604706\n",
      "Epoch 11, Iteration 500, Loss: 0.05186816304922104\n",
      "Epoch 11, Iteration 510, Loss: 0.022565707564353943\n",
      "Epoch 11, Iteration 520, Loss: 0.04418386146426201\n",
      "Epoch 11, Iteration 530, Loss: 0.06140681728720665\n",
      "Epoch 11, Iteration 540, Loss: 0.06413829326629639\n",
      "Epoch 11, Iteration 550, Loss: 0.03897611424326897\n",
      "Epoch 11, Iteration 560, Loss: 0.027345946058630943\n",
      "Epoch 11, Iteration 570, Loss: 0.029017390683293343\n",
      "Epoch 11, Iteration 580, Loss: 0.03718005120754242\n",
      "Epoch 11, Iteration 590, Loss: 0.0470547191798687\n",
      "Epoch 11, Iteration 600, Loss: 0.06356097012758255\n",
      "Epoch 11, Iteration 610, Loss: 0.0449267216026783\n",
      "Epoch 11, Iteration 620, Loss: 0.02694118209183216\n",
      "Epoch 11, Iteration 630, Loss: 0.055803317576646805\n",
      "Epoch 11, Iteration 640, Loss: 0.030390456318855286\n",
      "Epoch 11, Iteration 650, Loss: 0.06063249334692955\n",
      "Epoch 11, Iteration 660, Loss: 0.03216549754142761\n",
      "Epoch 11, Iteration 670, Loss: 0.07689984142780304\n",
      "Epoch 11, Iteration 680, Loss: 0.05428522825241089\n",
      "Epoch 11, Iteration 690, Loss: 0.05295104905962944\n",
      "Epoch 11, Iteration 700, Loss: 0.040388476103544235\n",
      "Epoch 11, Iteration 710, Loss: 0.057409558445215225\n",
      "Epoch 11, Iteration 720, Loss: 0.024851959198713303\n",
      "Epoch 11, Iteration 730, Loss: 0.030101515352725983\n",
      "Epoch 11, Validation Loss: 0.04231297714716714\n",
      "Validation loss decreased (0.042379 --> 0.042313). Saving model...\n",
      "Epoch 12, Iteration 0, Loss: 0.03560503199696541\n",
      "Epoch 12, Iteration 10, Loss: 0.047918759286403656\n",
      "Epoch 12, Iteration 20, Loss: 0.05610794574022293\n",
      "Epoch 12, Iteration 30, Loss: 0.042910147458314896\n",
      "Epoch 12, Iteration 40, Loss: 0.02605723775923252\n",
      "Epoch 12, Iteration 50, Loss: 0.05510533228516579\n",
      "Epoch 12, Iteration 60, Loss: 0.04803687706589699\n",
      "Epoch 12, Iteration 70, Loss: 0.03317662701010704\n",
      "Epoch 12, Iteration 80, Loss: 0.0675475001335144\n",
      "Epoch 12, Iteration 90, Loss: 0.0700015276670456\n",
      "Epoch 12, Iteration 100, Loss: 0.052128806710243225\n",
      "Epoch 12, Iteration 110, Loss: 0.045313697308301926\n",
      "Epoch 12, Iteration 120, Loss: 0.029047854244709015\n",
      "Epoch 12, Iteration 130, Loss: 0.053758855909109116\n",
      "Epoch 12, Iteration 140, Loss: 0.045884519815444946\n",
      "Epoch 12, Iteration 150, Loss: 0.050045520067214966\n",
      "Epoch 12, Iteration 160, Loss: 0.04264220967888832\n",
      "Epoch 12, Iteration 170, Loss: 0.040738966315984726\n",
      "Epoch 12, Iteration 180, Loss: 0.027908483520150185\n",
      "Epoch 12, Iteration 190, Loss: 0.04460935667157173\n",
      "Epoch 12, Iteration 200, Loss: 0.04512248933315277\n",
      "Epoch 12, Iteration 210, Loss: 0.053374215960502625\n",
      "Epoch 12, Iteration 220, Loss: 0.02724975161254406\n",
      "Epoch 12, Iteration 230, Loss: 0.02131558209657669\n",
      "Epoch 12, Iteration 240, Loss: 0.056697338819503784\n",
      "Epoch 12, Iteration 250, Loss: 0.02439144439995289\n",
      "Epoch 12, Iteration 260, Loss: 0.051312364637851715\n",
      "Epoch 12, Iteration 270, Loss: 0.04135226458311081\n",
      "Epoch 12, Iteration 280, Loss: 0.027062324807047844\n",
      "Epoch 12, Iteration 290, Loss: 0.025271611288189888\n",
      "Epoch 12, Iteration 300, Loss: 0.032330822199583054\n",
      "Epoch 12, Iteration 310, Loss: 0.05536573380231857\n",
      "Epoch 12, Iteration 320, Loss: 0.06498657912015915\n",
      "Epoch 12, Iteration 330, Loss: 0.03120320290327072\n",
      "Epoch 12, Iteration 340, Loss: 0.06341756135225296\n",
      "Epoch 12, Iteration 350, Loss: 0.03891253098845482\n",
      "Epoch 12, Iteration 360, Loss: 0.041573863476514816\n",
      "Epoch 12, Iteration 370, Loss: 0.03059990704059601\n",
      "Epoch 12, Iteration 380, Loss: 0.017431290820240974\n",
      "Epoch 12, Iteration 390, Loss: 0.03610531613230705\n",
      "Epoch 12, Iteration 400, Loss: 0.057297997176647186\n",
      "Epoch 12, Iteration 410, Loss: 0.0542883537709713\n",
      "Epoch 12, Iteration 420, Loss: 0.01990237645804882\n",
      "Epoch 12, Iteration 430, Loss: 0.012815418653190136\n",
      "Epoch 12, Iteration 440, Loss: 0.04624413326382637\n",
      "Epoch 12, Iteration 450, Loss: 0.021105296909809113\n",
      "Epoch 12, Iteration 460, Loss: 0.03305401653051376\n",
      "Epoch 12, Iteration 470, Loss: 0.03027426451444626\n",
      "Epoch 12, Iteration 480, Loss: 0.07939402759075165\n",
      "Epoch 12, Iteration 490, Loss: 0.03346765413880348\n",
      "Epoch 12, Iteration 500, Loss: 0.042178329080343246\n",
      "Epoch 12, Iteration 510, Loss: 0.08266360312700272\n",
      "Epoch 12, Iteration 520, Loss: 0.0385308563709259\n",
      "Epoch 12, Iteration 530, Loss: 0.029560936614871025\n",
      "Epoch 12, Iteration 540, Loss: 0.014886895194649696\n",
      "Epoch 12, Iteration 550, Loss: 0.04104101285338402\n",
      "Epoch 12, Iteration 560, Loss: 0.02299804985523224\n",
      "Epoch 12, Iteration 570, Loss: 0.11965015530586243\n",
      "Epoch 12, Iteration 580, Loss: 0.13728009164333344\n",
      "Epoch 12, Iteration 590, Loss: 0.0454007126390934\n",
      "Epoch 12, Iteration 600, Loss: 0.03938521817326546\n",
      "Epoch 12, Iteration 610, Loss: 0.02863261289894581\n",
      "Epoch 12, Iteration 620, Loss: 0.05967126786708832\n",
      "Epoch 12, Iteration 630, Loss: 0.04005603492259979\n",
      "Epoch 12, Iteration 640, Loss: 0.03387992084026337\n",
      "Epoch 12, Iteration 650, Loss: 0.04763640835881233\n",
      "Epoch 12, Iteration 660, Loss: 0.052770208567380905\n",
      "Epoch 12, Iteration 670, Loss: 0.05707958713173866\n",
      "Epoch 12, Iteration 680, Loss: 0.024400493130087852\n",
      "Epoch 12, Iteration 690, Loss: 0.02919578179717064\n",
      "Epoch 12, Iteration 700, Loss: 0.041127000004053116\n",
      "Epoch 12, Iteration 710, Loss: 0.028022723272442818\n",
      "Epoch 12, Iteration 720, Loss: 0.035548675805330276\n",
      "Epoch 12, Iteration 730, Loss: 0.0355072021484375\n",
      "Epoch 12, Validation Loss: 0.04230432096949738\n",
      "Validation loss decreased (0.042313 --> 0.042304). Saving model...\n",
      "Epoch 13, Iteration 0, Loss: 0.02129022218286991\n",
      "Epoch 13, Iteration 10, Loss: 0.04000352323055267\n",
      "Epoch 13, Iteration 20, Loss: 0.040555913001298904\n",
      "Epoch 13, Iteration 30, Loss: 0.03749990090727806\n",
      "Epoch 13, Iteration 40, Loss: 0.026315825060009956\n",
      "Epoch 13, Iteration 50, Loss: 0.0346696712076664\n",
      "Epoch 13, Iteration 60, Loss: 0.04026634246110916\n",
      "Epoch 13, Iteration 70, Loss: 0.03788308799266815\n",
      "Epoch 13, Iteration 80, Loss: 0.04216862842440605\n",
      "Epoch 13, Iteration 90, Loss: 0.016522808000445366\n",
      "Epoch 13, Iteration 100, Loss: 0.04719309136271477\n",
      "Epoch 13, Iteration 110, Loss: 0.04085719212889671\n",
      "Epoch 13, Iteration 120, Loss: 0.021984441205859184\n",
      "Epoch 13, Iteration 130, Loss: 0.04557393118739128\n",
      "Epoch 13, Iteration 140, Loss: 0.03928007557988167\n",
      "Epoch 13, Iteration 150, Loss: 0.03539348766207695\n",
      "Epoch 13, Iteration 160, Loss: 0.0961441844701767\n",
      "Epoch 13, Iteration 170, Loss: 0.029247252270579338\n",
      "Epoch 13, Iteration 180, Loss: 0.03846261277794838\n",
      "Epoch 13, Iteration 190, Loss: 0.03898504376411438\n",
      "Epoch 13, Iteration 200, Loss: 0.018634390085935593\n",
      "Epoch 13, Iteration 210, Loss: 0.037558767944574356\n",
      "Epoch 13, Iteration 220, Loss: 0.03458089008927345\n",
      "Epoch 13, Iteration 230, Loss: 0.05630071833729744\n",
      "Epoch 13, Iteration 240, Loss: 0.042375434190034866\n",
      "Epoch 13, Iteration 250, Loss: 0.04165204241871834\n",
      "Epoch 13, Iteration 260, Loss: 0.040702130645513535\n",
      "Epoch 13, Iteration 270, Loss: 0.05575326830148697\n",
      "Epoch 13, Iteration 280, Loss: 0.038039762526750565\n",
      "Epoch 13, Iteration 290, Loss: 0.08079809695482254\n",
      "Epoch 13, Iteration 300, Loss: 0.020910214632749557\n",
      "Epoch 13, Iteration 310, Loss: 0.045316342264413834\n",
      "Epoch 13, Iteration 320, Loss: 0.029332894831895828\n",
      "Epoch 13, Iteration 330, Loss: 0.061762094497680664\n",
      "Epoch 13, Iteration 340, Loss: 0.01517082005739212\n",
      "Epoch 13, Iteration 350, Loss: 0.06608252227306366\n",
      "Epoch 13, Iteration 360, Loss: 0.022151922807097435\n",
      "Epoch 13, Iteration 370, Loss: 0.05341168865561485\n",
      "Epoch 13, Iteration 380, Loss: 0.03943647816777229\n",
      "Epoch 13, Iteration 390, Loss: 0.03417729586362839\n",
      "Epoch 13, Iteration 400, Loss: 0.08389206975698471\n",
      "Epoch 13, Iteration 410, Loss: 0.0587589330971241\n",
      "Epoch 13, Iteration 420, Loss: 0.030739987269043922\n",
      "Epoch 13, Iteration 430, Loss: 0.04900377243757248\n",
      "Epoch 13, Iteration 440, Loss: 0.048349685966968536\n",
      "Epoch 13, Iteration 450, Loss: 0.05133267864584923\n",
      "Epoch 13, Iteration 460, Loss: 0.034327972680330276\n",
      "Epoch 13, Iteration 470, Loss: 0.039157476276159286\n",
      "Epoch 13, Iteration 480, Loss: 0.056897666305303574\n",
      "Epoch 13, Iteration 490, Loss: 0.03739934414625168\n",
      "Epoch 13, Iteration 500, Loss: 0.03445453941822052\n",
      "Epoch 13, Iteration 510, Loss: 0.04574880376458168\n",
      "Epoch 13, Iteration 520, Loss: 0.04195954278111458\n",
      "Epoch 13, Iteration 530, Loss: 0.09033731371164322\n",
      "Epoch 13, Iteration 540, Loss: 0.07928082346916199\n",
      "Epoch 13, Iteration 550, Loss: 0.04348571226000786\n",
      "Epoch 13, Iteration 560, Loss: 0.06596703827381134\n",
      "Epoch 13, Iteration 570, Loss: 0.029324932023882866\n",
      "Epoch 13, Iteration 580, Loss: 0.03720197081565857\n",
      "Epoch 13, Iteration 590, Loss: 0.09869810938835144\n",
      "Epoch 13, Iteration 600, Loss: 0.080559641122818\n",
      "Epoch 13, Iteration 610, Loss: 0.0679292306303978\n",
      "Epoch 13, Iteration 620, Loss: 0.02666943334043026\n",
      "Epoch 13, Iteration 630, Loss: 0.04463132843375206\n",
      "Epoch 13, Iteration 640, Loss: 0.10323718190193176\n",
      "Epoch 13, Iteration 650, Loss: 0.02824469283223152\n",
      "Epoch 13, Iteration 660, Loss: 0.04652823507785797\n",
      "Epoch 13, Iteration 670, Loss: 0.03183902055025101\n",
      "Epoch 13, Iteration 680, Loss: 0.02742159739136696\n",
      "Epoch 13, Iteration 690, Loss: 0.0234962347894907\n",
      "Epoch 13, Iteration 700, Loss: 0.04556365683674812\n",
      "Epoch 13, Iteration 710, Loss: 0.029656987637281418\n",
      "Epoch 13, Iteration 720, Loss: 0.04059099778532982\n",
      "Epoch 13, Iteration 730, Loss: 0.04507835954427719\n",
      "Epoch 13, Validation Loss: 0.04229454969501366\n",
      "Validation loss decreased (0.042304 --> 0.042295). Saving model...\n",
      "Epoch 14, Iteration 0, Loss: 0.048210661858320236\n",
      "Epoch 14, Iteration 10, Loss: 0.04426859691739082\n",
      "Epoch 14, Iteration 20, Loss: 0.031624261289834976\n",
      "Epoch 14, Iteration 30, Loss: 0.050411760807037354\n",
      "Epoch 14, Iteration 40, Loss: 0.03260607644915581\n",
      "Epoch 14, Iteration 50, Loss: 0.05529260262846947\n",
      "Epoch 14, Iteration 60, Loss: 0.06851411610841751\n",
      "Epoch 14, Iteration 70, Loss: 0.026477662846446037\n",
      "Epoch 14, Iteration 80, Loss: 0.02674606256186962\n",
      "Epoch 14, Iteration 90, Loss: 0.028854800388216972\n",
      "Epoch 14, Iteration 100, Loss: 0.03715519234538078\n",
      "Epoch 14, Iteration 110, Loss: 0.04002717509865761\n",
      "Epoch 14, Iteration 120, Loss: 0.026732875034213066\n",
      "Epoch 14, Iteration 130, Loss: 0.0374913364648819\n",
      "Epoch 14, Iteration 140, Loss: 0.049486737698316574\n",
      "Epoch 14, Iteration 150, Loss: 0.0616937130689621\n",
      "Epoch 14, Iteration 160, Loss: 0.041304443031549454\n",
      "Epoch 14, Iteration 170, Loss: 0.04567895829677582\n",
      "Epoch 14, Iteration 180, Loss: 0.057488951832056046\n",
      "Epoch 14, Iteration 190, Loss: 0.02415369264781475\n",
      "Epoch 14, Iteration 200, Loss: 0.0412040576338768\n",
      "Epoch 14, Iteration 210, Loss: 0.03950773924589157\n",
      "Epoch 14, Iteration 220, Loss: 0.06167767941951752\n",
      "Epoch 14, Iteration 230, Loss: 0.04301247000694275\n",
      "Epoch 14, Iteration 240, Loss: 0.04065529257059097\n",
      "Epoch 14, Iteration 250, Loss: 0.03337657451629639\n",
      "Epoch 14, Iteration 260, Loss: 0.043386783450841904\n",
      "Epoch 14, Iteration 270, Loss: 0.034761279821395874\n",
      "Epoch 14, Iteration 280, Loss: 0.050825610756874084\n",
      "Epoch 14, Iteration 290, Loss: 0.04186917096376419\n",
      "Epoch 14, Iteration 300, Loss: 0.0388050302863121\n",
      "Epoch 14, Iteration 310, Loss: 0.029845217242836952\n",
      "Epoch 14, Iteration 320, Loss: 0.041548486799001694\n",
      "Epoch 14, Iteration 330, Loss: 0.03831510618329048\n",
      "Epoch 14, Iteration 340, Loss: 0.07895497232675552\n",
      "Epoch 14, Iteration 350, Loss: 0.037093114107847214\n",
      "Epoch 14, Iteration 360, Loss: 0.025412146002054214\n",
      "Epoch 14, Iteration 370, Loss: 0.04615933075547218\n",
      "Epoch 14, Iteration 380, Loss: 0.05508218705654144\n",
      "Epoch 14, Iteration 390, Loss: 0.046183399856090546\n",
      "Epoch 14, Iteration 400, Loss: 0.01893582195043564\n",
      "Epoch 14, Iteration 410, Loss: 0.06280677020549774\n",
      "Epoch 14, Iteration 420, Loss: 0.022310912609100342\n",
      "Epoch 14, Iteration 430, Loss: 0.032428253442049026\n",
      "Epoch 14, Iteration 440, Loss: 0.031000524759292603\n",
      "Epoch 14, Iteration 450, Loss: 0.021618811413645744\n",
      "Epoch 14, Iteration 460, Loss: 0.05891144275665283\n",
      "Epoch 14, Iteration 470, Loss: 0.0175486970692873\n",
      "Epoch 14, Iteration 480, Loss: 0.0615270659327507\n",
      "Epoch 14, Iteration 490, Loss: 0.035760343074798584\n",
      "Epoch 14, Iteration 500, Loss: 0.1284288614988327\n",
      "Epoch 14, Iteration 510, Loss: 0.01621115207672119\n",
      "Epoch 14, Iteration 520, Loss: 0.04845062643289566\n",
      "Epoch 14, Iteration 530, Loss: 0.04950045421719551\n",
      "Epoch 14, Iteration 540, Loss: 0.04095577821135521\n",
      "Epoch 14, Iteration 550, Loss: 0.05033285543322563\n",
      "Epoch 14, Iteration 560, Loss: 0.035522349178791046\n",
      "Epoch 14, Iteration 570, Loss: 0.06270863860845566\n",
      "Epoch 14, Iteration 580, Loss: 0.04993823170661926\n",
      "Epoch 14, Iteration 590, Loss: 0.03416267782449722\n",
      "Epoch 14, Iteration 600, Loss: 0.03930079564452171\n",
      "Epoch 14, Iteration 610, Loss: 0.05745546892285347\n",
      "Epoch 14, Iteration 620, Loss: 0.018144723027944565\n",
      "Epoch 14, Iteration 630, Loss: 0.021528096869587898\n",
      "Epoch 14, Iteration 640, Loss: 0.03894446790218353\n",
      "Epoch 14, Iteration 650, Loss: 0.03162217512726784\n",
      "Epoch 14, Iteration 660, Loss: 0.06914369016885757\n",
      "Epoch 14, Iteration 670, Loss: 0.04360685870051384\n",
      "Epoch 14, Iteration 680, Loss: 0.030722353607416153\n",
      "Epoch 14, Iteration 690, Loss: 0.05045078694820404\n",
      "Epoch 14, Iteration 700, Loss: 0.06793761998414993\n",
      "Epoch 14, Iteration 710, Loss: 0.05201999470591545\n",
      "Epoch 14, Iteration 720, Loss: 0.04555350914597511\n",
      "Epoch 14, Iteration 730, Loss: 0.03551353141665459\n",
      "Epoch 14, Validation Loss: 0.04229197612680171\n",
      "Validation loss decreased (0.042295 --> 0.042292). Saving model...\n",
      "Epoch 15, Iteration 0, Loss: 0.06003223732113838\n",
      "Epoch 15, Iteration 10, Loss: 0.04948866367340088\n",
      "Epoch 15, Iteration 20, Loss: 0.0341079905629158\n",
      "Epoch 15, Iteration 30, Loss: 0.028770463541150093\n",
      "Epoch 15, Iteration 40, Loss: 0.04726197570562363\n",
      "Epoch 15, Iteration 50, Loss: 0.032554734498262405\n",
      "Epoch 15, Iteration 60, Loss: 0.02698558010160923\n",
      "Epoch 15, Iteration 70, Loss: 0.044539861381053925\n",
      "Epoch 15, Iteration 80, Loss: 0.03247211501002312\n",
      "Epoch 15, Iteration 90, Loss: 0.08706855028867722\n",
      "Epoch 15, Iteration 100, Loss: 0.04047596454620361\n",
      "Epoch 15, Iteration 110, Loss: 0.03270524740219116\n",
      "Epoch 15, Iteration 120, Loss: 0.05558858439326286\n",
      "Epoch 15, Iteration 130, Loss: 0.036513641476631165\n",
      "Epoch 15, Iteration 140, Loss: 0.03547900170087814\n",
      "Epoch 15, Iteration 150, Loss: 0.030772924423217773\n",
      "Epoch 15, Iteration 160, Loss: 0.049613986164331436\n",
      "Epoch 15, Iteration 170, Loss: 0.021076304838061333\n",
      "Epoch 15, Iteration 180, Loss: 0.05355217680335045\n",
      "Epoch 15, Iteration 190, Loss: 0.04016844555735588\n",
      "Epoch 15, Iteration 200, Loss: 0.16445444524288177\n",
      "Epoch 15, Iteration 210, Loss: 0.041172534227371216\n",
      "Epoch 15, Iteration 220, Loss: 0.023488087579607964\n",
      "Epoch 15, Iteration 230, Loss: 0.07900809496641159\n",
      "Epoch 15, Iteration 240, Loss: 0.10533539950847626\n",
      "Epoch 15, Iteration 250, Loss: 0.029469354078173637\n",
      "Epoch 15, Iteration 260, Loss: 0.03282070904970169\n",
      "Epoch 15, Iteration 270, Loss: 0.05119207873940468\n",
      "Epoch 15, Iteration 280, Loss: 0.04214439168572426\n",
      "Epoch 15, Iteration 290, Loss: 0.05564570054411888\n",
      "Epoch 15, Iteration 300, Loss: 0.03764456883072853\n",
      "Epoch 15, Iteration 310, Loss: 0.067489854991436\n",
      "Epoch 15, Iteration 320, Loss: 0.03537295013666153\n",
      "Epoch 15, Iteration 330, Loss: 0.026239464059472084\n",
      "Epoch 15, Iteration 340, Loss: 0.015797793865203857\n",
      "Epoch 15, Iteration 350, Loss: 0.0825762152671814\n",
      "Epoch 15, Iteration 360, Loss: 0.028480105102062225\n",
      "Epoch 15, Iteration 370, Loss: 0.03679164871573448\n",
      "Epoch 15, Iteration 380, Loss: 0.03766745328903198\n",
      "Epoch 15, Iteration 390, Loss: 0.0357947014272213\n",
      "Epoch 15, Iteration 400, Loss: 0.03144707530736923\n",
      "Epoch 15, Iteration 410, Loss: 0.04597821086645126\n",
      "Epoch 15, Iteration 420, Loss: 0.09806252270936966\n",
      "Epoch 15, Iteration 430, Loss: 0.034802183508872986\n",
      "Epoch 15, Iteration 440, Loss: 0.13013988733291626\n",
      "Epoch 15, Iteration 450, Loss: 0.030694155022501945\n",
      "Epoch 15, Iteration 460, Loss: 0.029693232849240303\n",
      "Epoch 15, Iteration 470, Loss: 0.03164729103446007\n",
      "Epoch 15, Iteration 480, Loss: 0.017068881541490555\n",
      "Epoch 15, Iteration 490, Loss: 0.021149663254618645\n",
      "Epoch 15, Iteration 500, Loss: 0.05277009680867195\n",
      "Epoch 15, Iteration 510, Loss: 0.03046002797782421\n",
      "Epoch 15, Iteration 520, Loss: 0.05784299224615097\n",
      "Epoch 15, Iteration 530, Loss: 0.0498286597430706\n",
      "Epoch 15, Iteration 540, Loss: 0.03554821014404297\n",
      "Epoch 15, Iteration 550, Loss: 0.048655103892087936\n",
      "Epoch 15, Iteration 560, Loss: 0.03305881842970848\n",
      "Epoch 15, Iteration 570, Loss: 0.07286514341831207\n",
      "Epoch 15, Iteration 580, Loss: 0.042449548840522766\n",
      "Epoch 15, Iteration 590, Loss: 0.03663777932524681\n",
      "Epoch 15, Iteration 600, Loss: 0.06260579824447632\n",
      "Epoch 15, Iteration 610, Loss: 0.03300422430038452\n",
      "Epoch 15, Iteration 620, Loss: 0.022945361211895943\n",
      "Epoch 15, Iteration 630, Loss: 0.04637613520026207\n",
      "Epoch 15, Iteration 640, Loss: 0.03142456337809563\n",
      "Epoch 15, Iteration 650, Loss: 0.05250120535492897\n",
      "Epoch 15, Iteration 660, Loss: 0.05764446780085564\n",
      "Epoch 15, Iteration 670, Loss: 0.05255904421210289\n",
      "Epoch 15, Iteration 680, Loss: 0.017199859023094177\n",
      "Epoch 15, Iteration 690, Loss: 0.041302360594272614\n",
      "Epoch 15, Iteration 700, Loss: 0.04865498095750809\n",
      "Epoch 15, Iteration 710, Loss: 0.022360723465681076\n",
      "Epoch 15, Iteration 720, Loss: 0.025557830929756165\n",
      "Epoch 15, Iteration 730, Loss: 0.03907391056418419\n",
      "Epoch 15, Validation Loss: 0.04225483773600148\n",
      "Validation loss decreased (0.042292 --> 0.042255). Saving model...\n",
      "Epoch 16, Iteration 0, Loss: 0.019743947312235832\n",
      "Epoch 16, Iteration 10, Loss: 0.040129948407411575\n",
      "Epoch 16, Iteration 20, Loss: 0.056374456733465195\n",
      "Epoch 16, Iteration 30, Loss: 0.03650330752134323\n",
      "Epoch 16, Iteration 40, Loss: 0.05145881697535515\n",
      "Epoch 16, Iteration 50, Loss: 0.03840987756848335\n",
      "Epoch 16, Iteration 60, Loss: 0.07313277572393417\n",
      "Epoch 16, Iteration 70, Loss: 0.017965976148843765\n",
      "Epoch 16, Iteration 80, Loss: 0.05506700277328491\n",
      "Epoch 16, Iteration 90, Loss: 0.05314724147319794\n",
      "Epoch 16, Iteration 100, Loss: 0.049416106194257736\n",
      "Epoch 16, Iteration 110, Loss: 0.04648784548044205\n",
      "Epoch 16, Iteration 120, Loss: 0.028583476319909096\n",
      "Epoch 16, Iteration 130, Loss: 0.07419038563966751\n",
      "Epoch 16, Iteration 140, Loss: 0.05297457054257393\n",
      "Epoch 16, Iteration 150, Loss: 0.06275487691164017\n",
      "Epoch 16, Iteration 160, Loss: 0.0361466147005558\n",
      "Epoch 16, Iteration 170, Loss: 0.031164830550551414\n",
      "Epoch 16, Iteration 180, Loss: 0.04706490784883499\n",
      "Epoch 16, Iteration 190, Loss: 0.037560053169727325\n",
      "Epoch 16, Iteration 200, Loss: 0.029871169477701187\n",
      "Epoch 16, Iteration 210, Loss: 0.029126787558197975\n",
      "Epoch 16, Iteration 220, Loss: 0.06049998477101326\n",
      "Epoch 16, Iteration 230, Loss: 0.09208305180072784\n",
      "Epoch 16, Iteration 240, Loss: 0.03099251538515091\n",
      "Epoch 16, Iteration 250, Loss: 0.02772834338247776\n",
      "Epoch 16, Iteration 260, Loss: 0.019553164020180702\n",
      "Epoch 16, Iteration 270, Loss: 0.07505156099796295\n",
      "Epoch 16, Iteration 280, Loss: 0.030856376513838768\n",
      "Epoch 16, Iteration 290, Loss: 0.04828432947397232\n",
      "Epoch 16, Iteration 300, Loss: 0.042360492050647736\n",
      "Epoch 16, Iteration 310, Loss: 0.03154996410012245\n",
      "Epoch 16, Iteration 320, Loss: 0.037651605904102325\n",
      "Epoch 16, Iteration 330, Loss: 0.016412414610385895\n",
      "Epoch 16, Iteration 340, Loss: 0.035564348101615906\n",
      "Epoch 16, Iteration 350, Loss: 0.039815422147512436\n",
      "Epoch 16, Iteration 360, Loss: 0.040701352059841156\n",
      "Epoch 16, Iteration 370, Loss: 0.03423421084880829\n",
      "Epoch 16, Iteration 380, Loss: 0.041812218725681305\n",
      "Epoch 16, Iteration 390, Loss: 0.042779698967933655\n",
      "Epoch 16, Iteration 400, Loss: 0.06503311544656754\n",
      "Epoch 16, Iteration 410, Loss: 0.06386302411556244\n",
      "Epoch 16, Iteration 420, Loss: 0.01927916146814823\n",
      "Epoch 16, Iteration 430, Loss: 0.02199414372444153\n",
      "Epoch 16, Iteration 440, Loss: 0.0505119264125824\n",
      "Epoch 16, Iteration 450, Loss: 0.0486934669315815\n",
      "Epoch 16, Iteration 460, Loss: 0.04326881468296051\n",
      "Epoch 16, Iteration 470, Loss: 0.06501305103302002\n",
      "Epoch 16, Iteration 480, Loss: 0.028345394879579544\n",
      "Epoch 16, Iteration 490, Loss: 0.027926595881581306\n",
      "Epoch 16, Iteration 500, Loss: 0.05068894475698471\n",
      "Epoch 16, Iteration 510, Loss: 0.04031796380877495\n",
      "Epoch 16, Iteration 520, Loss: 0.04043378308415413\n",
      "Epoch 16, Iteration 530, Loss: 0.05640773847699165\n",
      "Epoch 16, Iteration 540, Loss: 0.06414609402418137\n",
      "Epoch 16, Iteration 550, Loss: 0.04097048565745354\n",
      "Epoch 16, Iteration 560, Loss: 0.025614526122808456\n",
      "Epoch 16, Iteration 570, Loss: 0.05856598913669586\n",
      "Epoch 16, Iteration 580, Loss: 0.07733118534088135\n",
      "Epoch 16, Iteration 590, Loss: 0.04564644396305084\n",
      "Epoch 16, Iteration 600, Loss: 0.0352962501347065\n",
      "Epoch 16, Iteration 610, Loss: 0.020543331280350685\n",
      "Epoch 16, Iteration 620, Loss: 0.05163399502635002\n",
      "Epoch 16, Iteration 630, Loss: 0.06570553779602051\n",
      "Epoch 16, Iteration 640, Loss: 0.027250036597251892\n",
      "Epoch 16, Iteration 650, Loss: 0.042674679309129715\n",
      "Epoch 16, Iteration 660, Loss: 0.027276189997792244\n",
      "Epoch 16, Iteration 670, Loss: 0.08020579814910889\n",
      "Epoch 16, Iteration 680, Loss: 0.04184084013104439\n",
      "Epoch 16, Iteration 690, Loss: 0.0704798698425293\n",
      "Epoch 16, Iteration 700, Loss: 0.024334551766514778\n",
      "Epoch 16, Iteration 710, Loss: 0.046174801886081696\n",
      "Epoch 16, Iteration 720, Loss: 0.03739554435014725\n",
      "Epoch 16, Iteration 730, Loss: 0.035463597625494\n",
      "Epoch 16, Validation Loss: 0.04224544824303492\n",
      "Validation loss decreased (0.042255 --> 0.042245). Saving model...\n",
      "Epoch 17, Iteration 0, Loss: 0.05867800861597061\n",
      "Epoch 17, Iteration 10, Loss: 0.029483256861567497\n",
      "Epoch 17, Iteration 20, Loss: 0.06597019731998444\n",
      "Epoch 17, Iteration 30, Loss: 0.021229349076747894\n",
      "Epoch 17, Iteration 40, Loss: 0.03724591061472893\n",
      "Epoch 17, Iteration 50, Loss: 0.08460735529661179\n",
      "Epoch 17, Iteration 60, Loss: 0.03152989223599434\n",
      "Epoch 17, Iteration 70, Loss: 0.026565223932266235\n",
      "Epoch 17, Iteration 80, Loss: 0.06950122117996216\n",
      "Epoch 17, Iteration 90, Loss: 0.06674645096063614\n",
      "Epoch 17, Iteration 100, Loss: 0.035377271473407745\n",
      "Epoch 17, Iteration 110, Loss: 0.06632611900568008\n",
      "Epoch 17, Iteration 120, Loss: 0.04989996179938316\n",
      "Epoch 17, Iteration 130, Loss: 0.05326737090945244\n",
      "Epoch 17, Iteration 140, Loss: 0.08027383685112\n",
      "Epoch 17, Iteration 150, Loss: 0.034339770674705505\n",
      "Epoch 17, Iteration 160, Loss: 0.03453974798321724\n",
      "Epoch 17, Iteration 170, Loss: 0.02726711332798004\n",
      "Epoch 17, Iteration 180, Loss: 0.03815304487943649\n",
      "Epoch 17, Iteration 190, Loss: 0.07841271162033081\n",
      "Epoch 17, Iteration 200, Loss: 0.04555445909500122\n",
      "Epoch 17, Iteration 210, Loss: 0.03517824783921242\n",
      "Epoch 17, Iteration 220, Loss: 0.049143001437187195\n",
      "Epoch 17, Iteration 230, Loss: 0.036122214049100876\n",
      "Epoch 17, Iteration 240, Loss: 0.025040950626134872\n",
      "Epoch 17, Iteration 250, Loss: 0.040823161602020264\n",
      "Epoch 17, Iteration 260, Loss: 0.037712227553129196\n",
      "Epoch 17, Iteration 270, Loss: 0.06824247539043427\n",
      "Epoch 17, Iteration 280, Loss: 0.03143984451889992\n",
      "Epoch 17, Iteration 290, Loss: 0.029865654185414314\n",
      "Epoch 17, Iteration 300, Loss: 0.017796296626329422\n",
      "Epoch 17, Iteration 310, Loss: 0.07528634369373322\n",
      "Epoch 17, Iteration 320, Loss: 0.051503684371709824\n",
      "Epoch 17, Iteration 330, Loss: 0.04274534061551094\n",
      "Epoch 17, Iteration 340, Loss: 0.03266428783535957\n",
      "Epoch 17, Iteration 350, Loss: 0.030718563124537468\n",
      "Epoch 17, Iteration 360, Loss: 0.0311012864112854\n",
      "Epoch 17, Iteration 370, Loss: 0.028758717700839043\n",
      "Epoch 17, Iteration 380, Loss: 0.03420376032590866\n",
      "Epoch 17, Iteration 390, Loss: 0.040597300976514816\n",
      "Epoch 17, Iteration 400, Loss: 0.05121342092752457\n",
      "Epoch 17, Iteration 410, Loss: 0.029908495023846626\n",
      "Epoch 17, Iteration 420, Loss: 0.03833971545100212\n",
      "Epoch 17, Iteration 430, Loss: 0.03733685985207558\n",
      "Epoch 17, Iteration 440, Loss: 0.06755295395851135\n",
      "Epoch 17, Iteration 450, Loss: 0.05936514586210251\n",
      "Epoch 17, Iteration 460, Loss: 0.025460029020905495\n",
      "Epoch 17, Iteration 470, Loss: 0.027847567573189735\n",
      "Epoch 17, Iteration 480, Loss: 0.03125092759728432\n",
      "Epoch 17, Iteration 490, Loss: 0.02647835575044155\n",
      "Epoch 17, Iteration 500, Loss: 0.0641859844326973\n",
      "Epoch 17, Iteration 510, Loss: 0.037199344485998154\n",
      "Epoch 17, Iteration 520, Loss: 0.06016697362065315\n",
      "Epoch 17, Iteration 530, Loss: 0.04567163437604904\n",
      "Epoch 17, Iteration 540, Loss: 0.03825945034623146\n",
      "Epoch 17, Iteration 550, Loss: 0.050844706594944\n",
      "Epoch 17, Iteration 560, Loss: 0.04920880123972893\n",
      "Epoch 17, Iteration 570, Loss: 0.02764023281633854\n",
      "Epoch 17, Iteration 580, Loss: 0.04671061784029007\n",
      "Epoch 17, Iteration 590, Loss: 0.050948526710271835\n",
      "Epoch 17, Iteration 600, Loss: 0.055769894272089005\n",
      "Epoch 17, Iteration 610, Loss: 0.09901518374681473\n",
      "Epoch 17, Iteration 620, Loss: 0.05727403238415718\n",
      "Epoch 17, Iteration 630, Loss: 0.04338570311665535\n",
      "Epoch 17, Iteration 640, Loss: 0.059744469821453094\n",
      "Epoch 17, Iteration 650, Loss: 0.06412535160779953\n",
      "Epoch 17, Iteration 660, Loss: 0.03217003494501114\n",
      "Epoch 17, Iteration 670, Loss: 0.04403519630432129\n",
      "Epoch 17, Iteration 680, Loss: 0.04607792943716049\n",
      "Epoch 17, Iteration 690, Loss: 0.048964373767375946\n",
      "Epoch 17, Iteration 700, Loss: 0.03916716203093529\n",
      "Epoch 17, Iteration 710, Loss: 0.025271965190768242\n",
      "Epoch 17, Iteration 720, Loss: 0.044758085161447525\n",
      "Epoch 17, Iteration 730, Loss: 0.021382512524724007\n",
      "Epoch 17, Validation Loss: 0.04223790513756483\n",
      "Validation loss decreased (0.042245 --> 0.042238). Saving model...\n",
      "Epoch 18, Iteration 0, Loss: 0.06255143135786057\n",
      "Epoch 18, Iteration 10, Loss: 0.025112232193350792\n",
      "Epoch 18, Iteration 20, Loss: 0.05039194971323013\n",
      "Epoch 18, Iteration 30, Loss: 0.021807467564940453\n",
      "Epoch 18, Iteration 40, Loss: 0.04657517001032829\n",
      "Epoch 18, Iteration 50, Loss: 0.10731416940689087\n",
      "Epoch 18, Iteration 60, Loss: 0.049246661365032196\n",
      "Epoch 18, Iteration 70, Loss: 0.03883105888962746\n",
      "Epoch 18, Iteration 80, Loss: 0.05029735714197159\n",
      "Epoch 18, Iteration 90, Loss: 0.08301911503076553\n",
      "Epoch 18, Iteration 100, Loss: 0.03167252242565155\n",
      "Epoch 18, Iteration 110, Loss: 0.047408148646354675\n",
      "Epoch 18, Iteration 120, Loss: 0.03649132698774338\n",
      "Epoch 18, Iteration 130, Loss: 0.05319995433092117\n",
      "Epoch 18, Iteration 140, Loss: 0.04047667607665062\n",
      "Epoch 18, Iteration 150, Loss: 0.058278270065784454\n",
      "Epoch 18, Iteration 160, Loss: 0.1002475917339325\n",
      "Epoch 18, Iteration 170, Loss: 0.05759177729487419\n",
      "Epoch 18, Iteration 180, Loss: 0.03253398463129997\n",
      "Epoch 18, Iteration 190, Loss: 0.021778399124741554\n",
      "Epoch 18, Iteration 200, Loss: 0.05644955486059189\n",
      "Epoch 18, Iteration 210, Loss: 0.07081618160009384\n",
      "Epoch 18, Iteration 220, Loss: 0.03180333971977234\n",
      "Epoch 18, Iteration 230, Loss: 0.07103320211172104\n",
      "Epoch 18, Iteration 240, Loss: 0.021074552088975906\n",
      "Epoch 18, Iteration 250, Loss: 0.028678320348262787\n",
      "Epoch 18, Iteration 260, Loss: 0.10042297095060349\n",
      "Epoch 18, Iteration 270, Loss: 0.05589897185564041\n",
      "Epoch 18, Iteration 280, Loss: 0.04049084335565567\n",
      "Epoch 18, Iteration 290, Loss: 0.07234890013933182\n",
      "Epoch 18, Iteration 300, Loss: 0.02777976542711258\n",
      "Epoch 18, Iteration 310, Loss: 0.02169344387948513\n",
      "Epoch 18, Iteration 320, Loss: 0.028230315074324608\n",
      "Epoch 18, Iteration 330, Loss: 0.10501827299594879\n",
      "Epoch 18, Iteration 340, Loss: 0.035630736500024796\n",
      "Epoch 18, Iteration 350, Loss: 0.055407483130693436\n",
      "Epoch 18, Iteration 360, Loss: 0.03289397060871124\n",
      "Epoch 18, Iteration 370, Loss: 0.06153954938054085\n",
      "Epoch 18, Iteration 380, Loss: 0.03636348620057106\n",
      "Epoch 18, Iteration 390, Loss: 0.05696249380707741\n",
      "Epoch 18, Iteration 400, Loss: 0.04987051337957382\n",
      "Epoch 18, Iteration 410, Loss: 0.036351218819618225\n",
      "Epoch 18, Iteration 420, Loss: 0.03411445394158363\n",
      "Epoch 18, Iteration 430, Loss: 0.03047361969947815\n",
      "Epoch 18, Iteration 440, Loss: 0.03820094093680382\n",
      "Epoch 18, Iteration 450, Loss: 0.029677778482437134\n",
      "Epoch 18, Iteration 460, Loss: 0.03866055980324745\n",
      "Epoch 18, Iteration 470, Loss: 0.04400106146931648\n",
      "Epoch 18, Iteration 480, Loss: 0.02188374288380146\n",
      "Epoch 18, Iteration 490, Loss: 0.03147309646010399\n",
      "Epoch 18, Iteration 500, Loss: 0.05985976383090019\n",
      "Epoch 18, Iteration 510, Loss: 0.05959990248084068\n",
      "Epoch 18, Iteration 520, Loss: 0.03952723741531372\n",
      "Epoch 18, Iteration 530, Loss: 0.03363063186407089\n",
      "Epoch 18, Iteration 540, Loss: 0.0657867044210434\n",
      "Epoch 18, Iteration 550, Loss: 0.04169226437807083\n",
      "Epoch 18, Iteration 560, Loss: 0.0239122174680233\n",
      "Epoch 18, Iteration 570, Loss: 0.031264983117580414\n",
      "Epoch 18, Iteration 580, Loss: 0.026635928079485893\n",
      "Epoch 18, Iteration 590, Loss: 0.06908637285232544\n",
      "Epoch 18, Iteration 600, Loss: 0.019113359972834587\n",
      "Epoch 18, Iteration 610, Loss: 0.03552683815360069\n",
      "Epoch 18, Iteration 620, Loss: 0.02250090427696705\n",
      "Epoch 18, Iteration 630, Loss: 0.07984501868486404\n",
      "Epoch 18, Iteration 640, Loss: 0.04836875572800636\n",
      "Epoch 18, Iteration 650, Loss: 0.0350944884121418\n",
      "Epoch 18, Iteration 660, Loss: 0.03729227930307388\n",
      "Epoch 18, Iteration 670, Loss: 0.07835566252470016\n",
      "Epoch 18, Iteration 680, Loss: 0.051043182611465454\n",
      "Epoch 18, Iteration 690, Loss: 0.026048289611935616\n",
      "Epoch 18, Iteration 700, Loss: 0.11850832402706146\n",
      "Epoch 18, Iteration 710, Loss: 0.03551115468144417\n",
      "Epoch 18, Iteration 720, Loss: 0.028483763337135315\n",
      "Epoch 18, Iteration 730, Loss: 0.045914988964796066\n",
      "Epoch 18, Validation Loss: 0.04219996809716458\n",
      "Validation loss decreased (0.042238 --> 0.042200). Saving model...\n",
      "Epoch 19, Iteration 0, Loss: 0.050942856818437576\n",
      "Epoch 19, Iteration 10, Loss: 0.042106833308935165\n",
      "Epoch 19, Iteration 20, Loss: 0.034311577677726746\n",
      "Epoch 19, Iteration 30, Loss: 0.07368200272321701\n",
      "Epoch 19, Iteration 40, Loss: 0.035783905535936356\n",
      "Epoch 19, Iteration 50, Loss: 0.029339799657464027\n",
      "Epoch 19, Iteration 60, Loss: 0.028108004480600357\n",
      "Epoch 19, Iteration 70, Loss: 0.050364769995212555\n",
      "Epoch 19, Iteration 80, Loss: 0.035296082496643066\n",
      "Epoch 19, Iteration 90, Loss: 0.01732858456671238\n",
      "Epoch 19, Iteration 100, Loss: 0.06117761507630348\n",
      "Epoch 19, Iteration 110, Loss: 0.05374908819794655\n",
      "Epoch 19, Iteration 120, Loss: 0.05174780637025833\n",
      "Epoch 19, Iteration 130, Loss: 0.026390591636300087\n",
      "Epoch 19, Iteration 140, Loss: 0.06530755758285522\n",
      "Epoch 19, Iteration 150, Loss: 0.04827315732836723\n",
      "Epoch 19, Iteration 160, Loss: 0.055681854486465454\n",
      "Epoch 19, Iteration 170, Loss: 0.045674167573451996\n",
      "Epoch 19, Iteration 180, Loss: 0.02627009153366089\n",
      "Epoch 19, Iteration 190, Loss: 0.06152499094605446\n",
      "Epoch 19, Iteration 200, Loss: 0.09907092899084091\n",
      "Epoch 19, Iteration 210, Loss: 0.02380485087633133\n",
      "Epoch 19, Iteration 220, Loss: 0.04495929926633835\n",
      "Epoch 19, Iteration 230, Loss: 0.03485458344221115\n",
      "Epoch 19, Iteration 240, Loss: 0.051592517644166946\n",
      "Epoch 19, Iteration 250, Loss: 0.03540702909231186\n",
      "Epoch 19, Iteration 260, Loss: 0.03782271593809128\n",
      "Epoch 19, Iteration 270, Loss: 0.03134956583380699\n",
      "Epoch 19, Iteration 280, Loss: 0.03946349769830704\n",
      "Epoch 19, Iteration 290, Loss: 0.04314785450696945\n",
      "Epoch 19, Iteration 300, Loss: 0.0474654845893383\n",
      "Epoch 19, Iteration 310, Loss: 0.05727649852633476\n",
      "Epoch 19, Iteration 320, Loss: 0.026170799508690834\n",
      "Epoch 19, Iteration 330, Loss: 0.03248078003525734\n",
      "Epoch 19, Iteration 340, Loss: 0.051421500742435455\n",
      "Epoch 19, Iteration 350, Loss: 0.06966253370046616\n",
      "Epoch 19, Iteration 360, Loss: 0.038315299898386\n",
      "Epoch 19, Iteration 370, Loss: 0.03763424977660179\n",
      "Epoch 19, Iteration 380, Loss: 0.111287422478199\n",
      "Epoch 19, Iteration 390, Loss: 0.014372183009982109\n",
      "Epoch 19, Iteration 400, Loss: 0.07322284579277039\n",
      "Epoch 19, Iteration 410, Loss: 0.042418692260980606\n",
      "Epoch 19, Iteration 420, Loss: 0.03772896155714989\n",
      "Epoch 19, Iteration 430, Loss: 0.03633435070514679\n",
      "Epoch 19, Iteration 440, Loss: 0.022910872474312782\n",
      "Epoch 19, Iteration 450, Loss: 0.047214098274707794\n",
      "Epoch 19, Iteration 460, Loss: 0.03236304968595505\n",
      "Epoch 19, Iteration 470, Loss: 0.03548391908407211\n",
      "Epoch 19, Iteration 480, Loss: 0.05876368284225464\n",
      "Epoch 19, Iteration 490, Loss: 0.026083175092935562\n",
      "Epoch 19, Iteration 500, Loss: 0.04179508611559868\n",
      "Epoch 19, Iteration 510, Loss: 0.059314049780368805\n",
      "Epoch 19, Iteration 520, Loss: 0.057935141026973724\n",
      "Epoch 19, Iteration 530, Loss: 0.033590927720069885\n",
      "Epoch 19, Iteration 540, Loss: 0.04548191279172897\n",
      "Epoch 19, Iteration 550, Loss: 0.03461611270904541\n",
      "Epoch 19, Iteration 560, Loss: 0.06674196571111679\n",
      "Epoch 19, Iteration 570, Loss: 0.03618687018752098\n",
      "Epoch 19, Iteration 580, Loss: 0.04985341802239418\n",
      "Epoch 19, Iteration 590, Loss: 0.03539002314209938\n",
      "Epoch 19, Iteration 600, Loss: 0.04624852165579796\n",
      "Epoch 19, Iteration 610, Loss: 0.06665976345539093\n",
      "Epoch 19, Iteration 620, Loss: 0.023198479786515236\n",
      "Epoch 19, Iteration 630, Loss: 0.061747677624225616\n",
      "Epoch 19, Iteration 640, Loss: 0.059641025960445404\n",
      "Epoch 19, Iteration 650, Loss: 0.04622044414281845\n",
      "Epoch 19, Iteration 660, Loss: 0.025386495515704155\n",
      "Epoch 19, Iteration 670, Loss: 0.031237272545695305\n",
      "Epoch 19, Iteration 680, Loss: 0.03686491772532463\n",
      "Epoch 19, Iteration 690, Loss: 0.03521374613046646\n",
      "Epoch 19, Iteration 700, Loss: 0.05222529172897339\n",
      "Epoch 19, Iteration 710, Loss: 0.025476347655057907\n",
      "Epoch 19, Iteration 720, Loss: 0.0505957193672657\n",
      "Epoch 19, Iteration 730, Loss: 0.054953910410404205\n",
      "Epoch 19, Validation Loss: 0.04220204286115325\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 20, Iteration 0, Loss: 0.046789586544036865\n",
      "Epoch 20, Iteration 10, Loss: 0.03829910233616829\n",
      "Epoch 20, Iteration 20, Loss: 0.06228513643145561\n",
      "Epoch 20, Iteration 30, Loss: 0.02589629404246807\n",
      "Epoch 20, Iteration 40, Loss: 0.05916207656264305\n",
      "Epoch 20, Iteration 50, Loss: 0.035186272114515305\n",
      "Epoch 20, Iteration 60, Loss: 0.0248190276324749\n",
      "Epoch 20, Iteration 70, Loss: 0.027701428160071373\n",
      "Epoch 20, Iteration 80, Loss: 0.03853001445531845\n",
      "Epoch 20, Iteration 90, Loss: 0.04711935296654701\n",
      "Epoch 20, Iteration 100, Loss: 0.03770352527499199\n",
      "Epoch 20, Iteration 110, Loss: 0.045934032648801804\n",
      "Epoch 20, Iteration 120, Loss: 0.07166663557291031\n",
      "Epoch 20, Iteration 130, Loss: 0.04498257860541344\n",
      "Epoch 20, Iteration 140, Loss: 0.02287287451326847\n",
      "Epoch 20, Iteration 150, Loss: 0.05390327051281929\n",
      "Epoch 20, Iteration 160, Loss: 0.019815286621451378\n",
      "Epoch 20, Iteration 170, Loss: 0.051310740411281586\n",
      "Epoch 20, Iteration 180, Loss: 0.06218135729432106\n",
      "Epoch 20, Iteration 190, Loss: 0.029922055080533028\n",
      "Epoch 20, Iteration 200, Loss: 0.03484869748353958\n",
      "Epoch 20, Iteration 210, Loss: 0.05983884260058403\n",
      "Epoch 20, Iteration 220, Loss: 0.04724714905023575\n",
      "Epoch 20, Iteration 230, Loss: 0.02973252162337303\n",
      "Epoch 20, Iteration 240, Loss: 0.02400253340601921\n",
      "Epoch 20, Iteration 250, Loss: 0.024685565382242203\n",
      "Epoch 20, Iteration 260, Loss: 0.03046191856265068\n",
      "Epoch 20, Iteration 270, Loss: 0.03788616880774498\n",
      "Epoch 20, Iteration 280, Loss: 0.03889063000679016\n",
      "Epoch 20, Iteration 290, Loss: 0.04560574144124985\n",
      "Epoch 20, Iteration 300, Loss: 0.02354542352259159\n",
      "Epoch 20, Iteration 310, Loss: 0.02251710742712021\n",
      "Epoch 20, Iteration 320, Loss: 0.0381685309112072\n",
      "Epoch 20, Iteration 330, Loss: 0.04288817569613457\n",
      "Epoch 20, Iteration 340, Loss: 0.05912330001592636\n",
      "Epoch 20, Iteration 350, Loss: 0.05229836329817772\n",
      "Epoch 20, Iteration 360, Loss: 0.0896306037902832\n",
      "Epoch 20, Iteration 370, Loss: 0.04569856822490692\n",
      "Epoch 20, Iteration 380, Loss: 0.040166985243558884\n",
      "Epoch 20, Iteration 390, Loss: 0.047620322555303574\n",
      "Epoch 20, Iteration 400, Loss: 0.03510913625359535\n",
      "Epoch 20, Iteration 410, Loss: 0.09371623396873474\n",
      "Epoch 20, Iteration 420, Loss: 0.03406038135290146\n",
      "Epoch 20, Iteration 430, Loss: 0.061917442828416824\n",
      "Epoch 20, Iteration 440, Loss: 0.06149055063724518\n",
      "Epoch 20, Iteration 450, Loss: 0.06093984842300415\n",
      "Epoch 20, Iteration 460, Loss: 0.042656321078538895\n",
      "Epoch 20, Iteration 470, Loss: 0.03193497285246849\n",
      "Epoch 20, Iteration 480, Loss: 0.047555189579725266\n",
      "Epoch 20, Iteration 490, Loss: 0.03146282210946083\n",
      "Epoch 20, Iteration 500, Loss: 0.029521798714995384\n",
      "Epoch 20, Iteration 510, Loss: 0.059029653668403625\n",
      "Epoch 20, Iteration 520, Loss: 0.04122703894972801\n",
      "Epoch 20, Iteration 530, Loss: 0.04939601197838783\n",
      "Epoch 20, Iteration 540, Loss: 0.023663653060793877\n",
      "Epoch 20, Iteration 550, Loss: 0.054030321538448334\n",
      "Epoch 20, Iteration 560, Loss: 0.04279354214668274\n",
      "Epoch 20, Iteration 570, Loss: 0.03694269806146622\n",
      "Epoch 20, Iteration 580, Loss: 0.056162986904382706\n",
      "Epoch 20, Iteration 590, Loss: 0.06842227280139923\n",
      "Epoch 20, Iteration 600, Loss: 0.04823916405439377\n",
      "Epoch 20, Iteration 610, Loss: 0.03422784432768822\n",
      "Epoch 20, Iteration 620, Loss: 0.036971356719732285\n",
      "Epoch 20, Iteration 630, Loss: 0.027400953695178032\n",
      "Epoch 20, Iteration 640, Loss: 0.06901111453771591\n",
      "Epoch 20, Iteration 650, Loss: 0.03422519937157631\n",
      "Epoch 20, Iteration 660, Loss: 0.04876147583127022\n",
      "Epoch 20, Iteration 670, Loss: 0.01935235783457756\n",
      "Epoch 20, Iteration 680, Loss: 0.0383247435092926\n",
      "Epoch 20, Iteration 690, Loss: 0.04474050924181938\n",
      "Epoch 20, Iteration 700, Loss: 0.03643063083291054\n",
      "Epoch 20, Iteration 710, Loss: 0.05255105346441269\n",
      "Epoch 20, Iteration 720, Loss: 0.0647248923778534\n",
      "Epoch 20, Iteration 730, Loss: 0.06328726559877396\n",
      "Epoch 20, Validation Loss: 0.042174442570008665\n",
      "Validation loss decreased (0.042200 --> 0.042174). Saving model...\n",
      "Epoch 21, Iteration 0, Loss: 0.05837956443428993\n",
      "Epoch 21, Iteration 10, Loss: 0.030524209141731262\n",
      "Epoch 21, Iteration 20, Loss: 0.02584368921816349\n",
      "Epoch 21, Iteration 30, Loss: 0.03650367259979248\n",
      "Epoch 21, Iteration 40, Loss: 0.08062061667442322\n",
      "Epoch 21, Iteration 50, Loss: 0.048691436648368835\n",
      "Epoch 21, Iteration 60, Loss: 0.018816540017724037\n",
      "Epoch 21, Iteration 70, Loss: 0.058272019028663635\n",
      "Epoch 21, Iteration 80, Loss: 0.03231334313750267\n",
      "Epoch 21, Iteration 90, Loss: 0.08574407547712326\n",
      "Epoch 21, Iteration 100, Loss: 0.022182386368513107\n",
      "Epoch 21, Iteration 110, Loss: 0.04064178094267845\n",
      "Epoch 21, Iteration 120, Loss: 0.04325340688228607\n",
      "Epoch 21, Iteration 130, Loss: 0.042255230247974396\n",
      "Epoch 21, Iteration 140, Loss: 0.03691379725933075\n",
      "Epoch 21, Iteration 150, Loss: 0.027276501059532166\n",
      "Epoch 21, Iteration 160, Loss: 0.08111633360385895\n",
      "Epoch 21, Iteration 170, Loss: 0.0553196519613266\n",
      "Epoch 21, Iteration 180, Loss: 0.01777781918644905\n",
      "Epoch 21, Iteration 190, Loss: 0.03756549581885338\n",
      "Epoch 21, Iteration 200, Loss: 0.054858215153217316\n",
      "Epoch 21, Iteration 210, Loss: 0.08920956403017044\n",
      "Epoch 21, Iteration 220, Loss: 0.03631904348731041\n",
      "Epoch 21, Iteration 230, Loss: 0.03162394464015961\n",
      "Epoch 21, Iteration 240, Loss: 0.08679047226905823\n",
      "Epoch 21, Iteration 250, Loss: 0.04910655692219734\n",
      "Epoch 21, Iteration 260, Loss: 0.041684769093990326\n",
      "Epoch 21, Iteration 270, Loss: 0.027772625908255577\n",
      "Epoch 21, Iteration 280, Loss: 0.036457356065511703\n",
      "Epoch 21, Iteration 290, Loss: 0.044565167278051376\n",
      "Epoch 21, Iteration 300, Loss: 0.06593751907348633\n",
      "Epoch 21, Iteration 310, Loss: 0.026614222675561905\n",
      "Epoch 21, Iteration 320, Loss: 0.04855171963572502\n",
      "Epoch 21, Iteration 330, Loss: 0.02413303218781948\n",
      "Epoch 21, Iteration 340, Loss: 0.04851542040705681\n",
      "Epoch 21, Iteration 350, Loss: 0.02678513154387474\n",
      "Epoch 21, Iteration 360, Loss: 0.02300724759697914\n",
      "Epoch 21, Iteration 370, Loss: 0.051059041172266006\n",
      "Epoch 21, Iteration 380, Loss: 0.0431334413588047\n",
      "Epoch 21, Iteration 390, Loss: 0.04367858171463013\n",
      "Epoch 21, Iteration 400, Loss: 0.0342365987598896\n",
      "Epoch 21, Iteration 410, Loss: 0.040313638746738434\n",
      "Epoch 21, Iteration 420, Loss: 0.015143978409469128\n",
      "Epoch 21, Iteration 430, Loss: 0.0467812605202198\n",
      "Epoch 21, Iteration 440, Loss: 0.014499248936772346\n",
      "Epoch 21, Iteration 450, Loss: 0.06129496917128563\n",
      "Epoch 21, Iteration 460, Loss: 0.03258044272661209\n",
      "Epoch 21, Iteration 470, Loss: 0.0427347794175148\n",
      "Epoch 21, Iteration 480, Loss: 0.05094999447464943\n",
      "Epoch 21, Iteration 490, Loss: 0.07148245722055435\n",
      "Epoch 21, Iteration 500, Loss: 0.055597711354494095\n",
      "Epoch 21, Iteration 510, Loss: 0.052469316869974136\n",
      "Epoch 21, Iteration 520, Loss: 0.023374903947114944\n",
      "Epoch 21, Iteration 530, Loss: 0.04097343981266022\n",
      "Epoch 21, Iteration 540, Loss: 0.03582720085978508\n",
      "Epoch 21, Iteration 550, Loss: 0.03507189825177193\n",
      "Epoch 21, Iteration 560, Loss: 0.05084432661533356\n",
      "Epoch 21, Iteration 570, Loss: 0.04008873179554939\n",
      "Epoch 21, Iteration 580, Loss: 0.030489696189761162\n",
      "Epoch 21, Iteration 590, Loss: 0.026383794844150543\n",
      "Epoch 21, Iteration 600, Loss: 0.04031626507639885\n",
      "Epoch 21, Iteration 610, Loss: 0.03694693371653557\n",
      "Epoch 21, Iteration 620, Loss: 0.031768329441547394\n",
      "Epoch 21, Iteration 630, Loss: 0.0334060974419117\n",
      "Epoch 21, Iteration 640, Loss: 0.03607269749045372\n",
      "Epoch 21, Iteration 650, Loss: 0.027066392824053764\n",
      "Epoch 21, Iteration 660, Loss: 0.01557666901499033\n",
      "Epoch 21, Iteration 670, Loss: 0.029955504462122917\n",
      "Epoch 21, Iteration 680, Loss: 0.025644557550549507\n",
      "Epoch 21, Iteration 690, Loss: 0.056111328303813934\n",
      "Epoch 21, Iteration 700, Loss: 0.027286717668175697\n",
      "Epoch 21, Iteration 710, Loss: 0.07201150059700012\n",
      "Epoch 21, Iteration 720, Loss: 0.052754271775484085\n",
      "Epoch 21, Iteration 730, Loss: 0.03825214132666588\n",
      "Epoch 21, Validation Loss: 0.042172987095039825\n",
      "Validation loss decreased (0.042174 --> 0.042173). Saving model...\n",
      "Epoch 22, Iteration 0, Loss: 0.052956003695726395\n",
      "Epoch 22, Iteration 10, Loss: 0.1271233856678009\n",
      "Epoch 22, Iteration 20, Loss: 0.04994012042880058\n",
      "Epoch 22, Iteration 30, Loss: 0.08452040702104568\n",
      "Epoch 22, Iteration 40, Loss: 0.02475295588374138\n",
      "Epoch 22, Iteration 50, Loss: 0.031251005828380585\n",
      "Epoch 22, Iteration 60, Loss: 0.05268409475684166\n",
      "Epoch 22, Iteration 70, Loss: 0.05317933112382889\n",
      "Epoch 22, Iteration 80, Loss: 0.0333201065659523\n",
      "Epoch 22, Iteration 90, Loss: 0.08208607882261276\n",
      "Epoch 22, Iteration 100, Loss: 0.044442035257816315\n",
      "Epoch 22, Iteration 110, Loss: 0.04248997941613197\n",
      "Epoch 22, Iteration 120, Loss: 0.06115156039595604\n",
      "Epoch 22, Iteration 130, Loss: 0.08181031048297882\n",
      "Epoch 22, Iteration 140, Loss: 0.04199768230319023\n",
      "Epoch 22, Iteration 150, Loss: 0.055891185998916626\n",
      "Epoch 22, Iteration 160, Loss: 0.03420817106962204\n",
      "Epoch 22, Iteration 170, Loss: 0.03312443196773529\n",
      "Epoch 22, Iteration 180, Loss: 0.04301969334483147\n",
      "Epoch 22, Iteration 190, Loss: 0.041377246379852295\n",
      "Epoch 22, Iteration 200, Loss: 0.03811141848564148\n",
      "Epoch 22, Iteration 210, Loss: 0.03319942578673363\n",
      "Epoch 22, Iteration 220, Loss: 0.024141786620020866\n",
      "Epoch 22, Iteration 230, Loss: 0.03899656981229782\n",
      "Epoch 22, Iteration 240, Loss: 0.04126874729990959\n",
      "Epoch 22, Iteration 250, Loss: 0.041940074414014816\n",
      "Epoch 22, Iteration 260, Loss: 0.025273501873016357\n",
      "Epoch 22, Iteration 270, Loss: 0.059338364750146866\n",
      "Epoch 22, Iteration 280, Loss: 0.041143517941236496\n",
      "Epoch 22, Iteration 290, Loss: 0.03693724796175957\n",
      "Epoch 22, Iteration 300, Loss: 0.03590600565075874\n",
      "Epoch 22, Iteration 310, Loss: 0.11928019672632217\n",
      "Epoch 22, Iteration 320, Loss: 0.0189451165497303\n",
      "Epoch 22, Iteration 330, Loss: 0.04021323472261429\n",
      "Epoch 22, Iteration 340, Loss: 0.02973669022321701\n",
      "Epoch 22, Iteration 350, Loss: 0.08317674696445465\n",
      "Epoch 22, Iteration 360, Loss: 0.08654733747243881\n",
      "Epoch 22, Iteration 370, Loss: 0.0517575666308403\n",
      "Epoch 22, Iteration 380, Loss: 0.07667748630046844\n",
      "Epoch 22, Iteration 390, Loss: 0.04541859030723572\n",
      "Epoch 22, Iteration 400, Loss: 0.0301931444555521\n",
      "Epoch 22, Iteration 410, Loss: 0.06676789373159409\n",
      "Epoch 22, Iteration 420, Loss: 0.024437569081783295\n",
      "Epoch 22, Iteration 430, Loss: 0.035326920449733734\n",
      "Epoch 22, Iteration 440, Loss: 0.037851732224226\n",
      "Epoch 22, Iteration 450, Loss: 0.05134377256035805\n",
      "Epoch 22, Iteration 460, Loss: 0.039523229002952576\n",
      "Epoch 22, Iteration 470, Loss: 0.07952374964952469\n",
      "Epoch 22, Iteration 480, Loss: 0.07401398569345474\n",
      "Epoch 22, Iteration 490, Loss: 0.061579637229442596\n",
      "Epoch 22, Iteration 500, Loss: 0.05387260764837265\n",
      "Epoch 22, Iteration 510, Loss: 0.045506760478019714\n",
      "Epoch 22, Iteration 520, Loss: 0.022625429555773735\n",
      "Epoch 22, Iteration 530, Loss: 0.0409010611474514\n",
      "Epoch 22, Iteration 540, Loss: 0.039945438504219055\n",
      "Epoch 22, Iteration 550, Loss: 0.06399128586053848\n",
      "Epoch 22, Iteration 560, Loss: 0.04926065728068352\n",
      "Epoch 22, Iteration 570, Loss: 0.011255737394094467\n",
      "Epoch 22, Iteration 580, Loss: 0.024873709306120872\n",
      "Epoch 22, Iteration 590, Loss: 0.044071994721889496\n",
      "Epoch 22, Iteration 600, Loss: 0.06586559861898422\n",
      "Epoch 22, Iteration 610, Loss: 0.04470903053879738\n",
      "Epoch 22, Iteration 620, Loss: 0.04628365859389305\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1463feeb5728>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0megg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Conda\\envs\\HiFi\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Conda\\envs\\HiFi\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Conda\\envs\\HiFi\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Conda\\envs\\HiFi\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Conda\\envs\\HiFi\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-1463feeb5728>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;31m# Load audio and EGG data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# None for native sampling rate, or replace with specific rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0megg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0megg_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[1;33m)\u001b[0m      \u001b[1;31m# Assume same sample rate as audio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;31m# Find the maximum length in the dataset or a predetermined max length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Conda\\envs\\HiFi\\lib\\site-packages\\librosa\\util\\decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Conda\\envs\\HiFi\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msr_native\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Conda\\envs\\HiFi\\lib\\site-packages\\librosa\\util\\decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Conda\\envs\\HiFi\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36mresample\u001b[1;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoxr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresampy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Conda\\envs\\HiFi\\lib\\site-packages\\resampy\\core.py\u001b[0m in \u001b[0;36mresample\u001b[1;34m(x, sr_orig, sr_new, axis, filter, parallel, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         )\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Conda\\envs\\HiFi\\lib\\site-packages\\numba\\np\\ufunc\\gufunc.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_ufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "samplerate = 16000\n",
    "\n",
    "# load data\n",
    "audio_path = r'F:\\Audio\\Audio'\n",
    "egg_path = r'F:\\Audio\\EGG'\n",
    "audio_list = os.listdir(audio_path)\n",
    "egg_list = os.listdir(egg_path)\n",
    "\n",
    "class AudioEGGDataset(Dataset):\n",
    "    def __init__(self, audio_path, egg_path, transform=None):\n",
    "        self.audio_path = audio_path\n",
    "        self.egg_path = egg_path\n",
    "        self.audio_list = os.listdir(audio_path)\n",
    "        self.egg_list = os.listdir(egg_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            audio_file = os.path.join(self.audio_path, self.audio_list[idx])\n",
    "            egg_file = os.path.join(self.egg_path, self.egg_list[idx])\n",
    "\n",
    "            # Load audio and EGG data\n",
    "            audio, sr = librosa.load(audio_file, sr=samplerate)  # None for native sampling rate, or replace with specific rate\n",
    "            egg, _ = librosa.load(egg_file, sr=samplerate)      # Assume same sample rate as audio\n",
    "\n",
    "            # Find the maximum length in the dataset or a predetermined max length\n",
    "            max_length = 160000  # This could also be dynamically calculated or set based on your data\n",
    "            # Pad or truncate to the maximum length\n",
    "            audio = librosa.util.fix_length(audio, size=max_length)\n",
    "            egg = librosa.util.fix_length(egg, size=max_length)\n",
    "\n",
    "            if self.transform:\n",
    "                audio = self.transform(audio)\n",
    "                egg = self.transform(egg)\n",
    "\n",
    "            # divide by max value to normalize\n",
    "            audio = audio / np.max(np.abs(audio))\n",
    "            egg = egg / np.max(np.abs(egg))\n",
    "\n",
    "            # Convert to PyTorch tensors and add channel dimension\n",
    "            audio = torch.from_numpy(audio).float().unsqueeze(0)  # Add channel dimension\n",
    "            egg = torch.from_numpy(egg).float().unsqueeze(0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {audio_file} and {egg_file}: {e}\")\n",
    "            return None\n",
    "\n",
    "        return audio, egg\n",
    "\n",
    "dataset = AudioEGGDataset(audio_path, egg_path)\n",
    "# Create train and validation and test sets\n",
    "batch_size = 2  # Adjust as necessary\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, input_channels, dilation_channels):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.receptive_field_size = 1\n",
    "        self.dilated_convs = nn.ModuleList()\n",
    "\n",
    "        dilations = [2**i for i in range(6)]\n",
    "        self.dilated_convs.append(nn.Conv1d(input_channels, 2 * dilation_channels, kernel_size=3, padding=dilations[0]))\n",
    "        for dilation in dilations[1:]:\n",
    "            padding = dilation * (3 - 1) // 2\n",
    "            self.dilated_convs.append(nn.Conv1d(dilation_channels, 2 * dilation_channels, kernel_size=3, padding=padding, dilation=dilation))\n",
    "            self.receptive_field_size += dilation * 2\n",
    "\n",
    "        self.output_conv = nn.Conv1d(dilation_channels, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.dilated_convs:\n",
    "            out = conv(x)\n",
    "            # Splitting the output of the convolution into filter and gate parts\n",
    "            filter, gate = torch.split(out, self.dilation_channels, dim=1)  # Correct dimension for splitting is 1 (channels)\n",
    "            x = torch.tanh(filter) * torch.sigmoid(gate)\n",
    "\n",
    "        return self.output_conv(x)\n",
    "\n",
    "# Instantiate the model\n",
    "channels = 32  # You may need to tune this based on your dataset\n",
    "model = WaveNet(input_channels=1, dilation_channels=channels)\n",
    "\n",
    "\n",
    "# cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "for epoch in range(100):  # Adjust the number of epochs based on your needs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        audio, egg = data\n",
    "        audio = audio.to(device)\n",
    "        egg = egg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(audio)\n",
    "        loss = criterion(output, egg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 10 == 0:  # Log every 10 batches\n",
    "            print(f'Epoch {epoch}, Iteration {i}, Loss: {loss.item()}')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            audio, egg = data\n",
    "            audio = audio.to(device)\n",
    "            egg = egg.to(device)\n",
    "            output = model(audio)\n",
    "            loss = criterion(output, egg)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "    val_loss = val_running_loss / len(val_dataloader)\n",
    "    print(f'Epoch {epoch}, Validation Loss: {val_loss}')\n",
    "\n",
    "    # Early stopping and saving best model based on validation loss\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % 10 == 0:  # Save every 10 epochs in chkpt folder\n",
    "        checkpoint_path = os.path.join('chkpt', f'checkpoint_{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss / len(dataloader),\n",
    "            'val_loss': val_loss,\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < early_stopping.val_loss_min:\n",
    "        best_model_path = 'WaveNetbest_model.pt'\n",
    "        torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        audio, egg = data\n",
    "        audio = audio.to(device)\n",
    "        egg = egg.to(device)\n",
    "        output = model(audio)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the first predicted EGG as wav\n",
    "output = output[0]\n",
    "# save wav in the current folder using soundfile\n",
    "import soundfile as sf\n",
    "sf.write('output.wav', output, samplerate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
