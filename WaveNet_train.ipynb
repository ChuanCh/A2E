{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from audio_process import *\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import optim\n",
    "\n",
    "samplerate = 16000\n",
    "# data path\n",
    "# SRP_audio_path = r'F:\\audio\\SRP_segmented\\Voice'\n",
    "# SRP_egg_path = r'F:\\audio\\SRP_segmented\\EGG'\n",
    "# VRP_audio_path = r'F:\\audio\\10s_segment\\VRP_segmented\\voice_test'\n",
    "# VRP_egg_path = r'F:\\audio\\10s_segment\\VRP_segmented\\egg_test'\n",
    "VRP_path = r'F:\\audio\\test_VRP_F02\\test_Voice_EGG.wav'\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_audio(audio, sr, frame_length_ms=12, hop_length_samples=1):\n",
    "\n",
    "    # Calculate frame length in samples\n",
    "    frame_length_samples = int(sr * frame_length_ms / 1000)\n",
    "\n",
    "    # Number of frames\n",
    "    num_frames = 1 + (len(audio) - frame_length_samples) // hop_length_samples\n",
    "\n",
    "    # Initialize an array to hold the frames\n",
    "    frames = np.zeros((num_frames, frame_length_samples))\n",
    "\n",
    "    # Segment audio\n",
    "    for i in range(num_frames):\n",
    "        start_sample = i * hop_length_samples\n",
    "        end_sample = start_sample + frame_length_samples\n",
    "        frames[i] = audio[start_sample:end_sample]\n",
    "\n",
    "    return frames\n",
    "\n",
    "class AudioEGGDataset(Dataset):\n",
    "    def __init__(self, audio_frames, egg_frames):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with pre-loaded data.\n",
    "        :param audio_frames: A list or array of preprocessed and segmented audio frames.\n",
    "        :param egg_frames: A list or array of preprocessed and segmented EGG frames.\n",
    "        :param transform: Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        assert len(audio_frames) == len(egg_frames), \"Audio and EGG frames must be the same length\"\n",
    "        self.audio_frames = audio_frames\n",
    "        self.egg_frames = egg_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_frame = self.audio_frames[idx]\n",
    "        egg_frame = self.egg_frames[idx]\n",
    "\n",
    "        # Convert arrays to PyTorch tensors\n",
    "        audio_tensor = torch.from_numpy(audio_frame).float().unsqueeze(0)  # Add channel dimension if needed\n",
    "        egg_tensor = torch.from_numpy(egg_frame).float().unsqueeze(0)      # Add channel dimension if needed\n",
    "\n",
    "        return audio_tensor, egg_tensor\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, input_channels, dilation_channels):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.receptive_field_size = 1\n",
    "        self.dilated_convs = nn.ModuleList()\n",
    "\n",
    "        dilations = [2**i for i in range(6)]\n",
    "        self.dilated_convs.append(nn.Conv1d(input_channels, 2 * dilation_channels, kernel_size=3, padding=dilations[0]))\n",
    "        for dilation in dilations[1:]:\n",
    "            padding = dilation * (3 - 1) // 2\n",
    "            self.dilated_convs.append(nn.Conv1d(dilation_channels, 2 * dilation_channels, kernel_size=3, padding=padding, dilation=dilation))\n",
    "            self.receptive_field_size += dilation * 2\n",
    "\n",
    "        self.output_conv = nn.Conv1d(dilation_channels, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.dilated_convs:\n",
    "            out = conv(x)\n",
    "            # Splitting the output of the convolution into filter and gate parts\n",
    "            filter, gate = torch.split(out, self.dilation_channels, dim=1)  # Correct dimension for splitting is 1 (channels)\n",
    "            x = torch.tanh(filter) * torch.sigmoid(gate)\n",
    "\n",
    "        return self.output_conv(x)\n",
    "    \n",
    "class CosineSimilarityLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineSimilarityLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # Normalize outputs and targets to unit vectors\n",
    "        outputs_norm = F.normalize(outputs, p=2, dim=1)\n",
    "        targets_norm = F.normalize(targets, p=2, dim=1)\n",
    "        # Compute cosine similarity\n",
    "        cosine_loss = 1 - torch.sum(outputs_norm * targets_norm, dim=1).mean()\n",
    "        return cosine_loss\n",
    "    \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.val_loss_min > val_loss:\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "            torch.save(model.state_dict(), 'checkpoint_model.pt')\n",
    "            self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "wave, sr = librosa.load(VRP_path, sr=samplerate, mono=False) \n",
    "audio = wave[0]\n",
    "egg = wave[1]\n",
    "audio = voice_preprocess(audio, samplerate)\n",
    "egg = process_EGG_signal(egg, samplerate)\n",
    "audio = audio / np.max(np.abs(audio))\n",
    "egg = egg / np.max(np.abs(egg))\n",
    "\n",
    "# segment audio\n",
    "audio_frames = segment_audio(audio, samplerate)\n",
    "egg_frames = segment_audio(egg, samplerate)\n",
    "\n",
    "print(audio_frames.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot audio and EGG frames first 10\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(audio_frames[135248])\n",
    "plt.title('Audio frame 0')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(egg_frames[135248])\n",
    "plt.title('EGG frame 0')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioEGGDataset(audio_frames, egg_frames)\n",
    "# Create train and validation and test sets\n",
    "batch_size = 512  # Adjust as necessary\n",
    "train_size = int(0.85 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dataset\n",
    "audio, egg = next(iter(dataloader))\n",
    "print(audio.shape, egg.shape)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(audio[240].squeeze())\n",
    "plt.title('Audio frame 0')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(egg[240].squeeze())\n",
    "plt.title('EGG frame 0')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "channels = 32  # You may need to tune this based on your dataset\n",
    "model = WaveNet(input_channels=1, dilation_channels=channels)\n",
    "\n",
    "# cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Instantiate the Cosine Similarity Loss\n",
    "criterion = CosineSimilarityLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "for epoch in range(100):  # Adjust the number of epochs based on your needs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        audio, egg = data\n",
    "        audio = audio.to(device)\n",
    "        egg = egg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(audio)\n",
    "        loss = criterion(output, egg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 10 == 0:  # Log every 10 batches\n",
    "            print(f'Epoch {epoch}, Iteration {i}, Loss: {loss.item()}')\n",
    "\n",
    "    average_loss = running_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch}, Average Training Loss: {average_loss}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            audio, egg = data\n",
    "            audio = audio.to(device)\n",
    "            egg = egg.to(device)\n",
    "            output = model(audio)\n",
    "            loss = criterion(output, egg)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "    val_loss = val_running_loss / len(val_dataloader)\n",
    "    print(f'Epoch {epoch}, Validation Loss: {val_loss}')\n",
    "\n",
    "    # Early stopping and saving best model based on validation loss\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % 10 == 0:  # Save every 10 epochs in chkpt folder\n",
    "        checkpoint_path = os.path.join('chkpt', f'checkpoint_{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss / len(dataloader),\n",
    "            'val_loss': val_loss,\n",
    "        }, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        audio, egg = data\n",
    "        audio = audio.to(device)\n",
    "        egg = egg.to(device)\n",
    "        output = model(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'WaveNet.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "egg = egg.cpu()\n",
    "output = output.cpu()\n",
    "\n",
    "\n",
    "# Plot the first sample\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(egg[1].squeeze(), label='Ground Truth')\n",
    "plt.plot(output[1].squeeze(), label='Prediction')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "# save the first predicted EGG as wav\n",
    "output_wav = output[0]\n",
    "# save wav in the current folder using soundfile\n",
    "\n",
    "sf.write('output.wav', output_wav.squeeze().numpy(), samplerate)\n",
    "\n",
    "# save the first ground truth EGG as wav\n",
    "egg_wav = egg[0]\n",
    "sf.write('egg.wav', egg_wav.squeeze().numpy(), samplerate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from audio_process import *\n",
    "from scipy.signal import firwin, filtfilt, medfilt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# load wav in dir named output_wav\n",
    "output_wav, sr = librosa.load('output_wav/WaveNet_output.wav', sr=samplerate)\n",
    "egg_wav, sr = librosa.load('output_wav/WaveNet_egg_FFT_filtered.wav', sr=samplerate)\n",
    "# smooth egg_wav\n",
    "def smooth_signal(signal, sample_rate=1000, cutoff_hz=50, numtaps=101):\n",
    "    # Design the low-pass FIR filter\n",
    "    fir_coeff = firwin(numtaps, cutoff_hz, fs=sample_rate, window='hamming', pass_zero=True)\n",
    "\n",
    "    # Apply the filter to the signal using filtfilt to avoid phase shift\n",
    "    smoothed_signal = filtfilt(fir_coeff, 1.0, signal)\n",
    "\n",
    "    return smoothed_signal\n",
    "\n",
    "def adjust_lower_part(prediction, ground_truth, threshold=-0.1):\n",
    "    # Find the indices where the prediction signal is below the threshold\n",
    "    indices = prediction < threshold\n",
    "\n",
    "    # Compute the average difference in the lower part of the signals\n",
    "    offset = np.mean(ground_truth[indices] - prediction[indices])\n",
    "\n",
    "    # Apply the offset to the prediction signal\n",
    "    adjusted_prediction = prediction + offset\n",
    "\n",
    "    return adjusted_prediction\n",
    "\n",
    "def low_pass_filter(signal, sample_rate=samplerate, cutoff_hz=50, numtaps=4):\n",
    "    # Design the low-pass FIR filter\n",
    "    fir_coeff = firwin(numtaps, cutoff_hz, fs=sample_rate, window='hamming', pass_zero=True)\n",
    "\n",
    "    # Apply the filter to the signal using filtfilt to avoid phase shift\n",
    "    filtered_signal = filtfilt(fir_coeff, 1.0, signal)\n",
    "\n",
    "    return filtered_signal\n",
    "\n",
    "output_wav = low_pass_filter(output_wav)\n",
    "\n",
    "# plot output and ground truth\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(output_wav[22000:22200], label='Prediction')\n",
    "plt.plot(egg_wav[22000:22200], label='Ground Truth')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the cycles in the output and ground truth\n",
    "def find_cycles(signal, threshold=0.1):\n",
    "    # Find the indices where the signal is above the threshold\n",
    "    indices = np.where(signal > threshold)[0]\n",
    "\n",
    "    # Compute the differences between consecutive indices\n",
    "    diffs = np.diff(indices)\n",
    "\n",
    "    # Find the indices where the differences are greater than 1\n",
    "    cycle_indices = np.where(diffs > 1)[0]\n",
    "\n",
    "    # Split the indices into cycles\n",
    "    cycles = np.split(indices, cycle_indices + 1)\n",
    "\n",
    "    return cycles\n",
    "\n",
    "output_cycles = find_cycles(output[0])\n",
    "egg_cycles = find_cycles(egg[0])\n",
    "\n",
    "def find_qci(EGG):\n",
    "    unit = unit_EGG(EGG)\n",
    "    # qci is the area under the curve of the unit EGG signal\n",
    "    qci = np.trapz(unit[1], unit[0])\n",
    "    return qci\n",
    "\n",
    "def unit_EGG(EGG):\n",
    "    '''\n",
    "    Input: each cycle of EGG signal\n",
    "    Output: unit EGG signal for computing qci\n",
    "    '''\n",
    "    EGG_shifted = EGG - np.min(EGG)\n",
    "    \n",
    "    # Normalize the amplitude to have a maximum of 1\n",
    "    normalized_amplitude = EGG_shifted / np.max(EGG_shifted)\n",
    "    \n",
    "    # Normalize the time axis\n",
    "    num_samples = len(EGG)\n",
    "    normalized_time = np.linspace(0, 1, num_samples, endpoint=False)\n",
    "    \n",
    "    return normalized_time, normalized_amplitude\n",
    "\n",
    "def find_dEGGmax(signal):\n",
    "    scaled_signal = signal * 32767\n",
    "    rounded_signal = np.round(scaled_signal).astype(np.int16)\n",
    "\n",
    "    # Check if the signal has sufficient length\n",
    "    if len(rounded_signal) < 2:\n",
    "        return np.nan  # or another appropriate value indicating an issue\n",
    "\n",
    "    # Find the largest positive difference dmax over the period\n",
    "    dmax = np.max(np.abs(np.diff(rounded_signal)))\n",
    "    \n",
    "    # Assuming a sinusoidal waveform with peak-to-peak amplitude Ap-p = 2\n",
    "    Ap_p = np.max(rounded_signal) - np.min(rounded_signal)\n",
    "    \n",
    "    # Check if Ap_p is non-zero to avoid division by zero\n",
    "    if Ap_p == 0:\n",
    "        return np.nan  # or another appropriate value indicating an issue\n",
    "\n",
    "    # Calculate QD\n",
    "    period_length_T = len(rounded_signal)\n",
    "    QD = 2 * dmax / (Ap_p * np.sin(2 * np.pi / period_length_T))\n",
    "    \n",
    "    return QD\n",
    "\n",
    "# in each cycle, calculate qci and dEGGmax\n",
    "output_qci = []\n",
    "output_dEGGmax = []\n",
    "for cycle in egg_cycles:\n",
    "    qci = find_qci(output[cycle])\n",
    "    dEGGmax = find_dEGGmax(output[cycle])\n",
    "    output_qci.append(qci)\n",
    "    output_dEGGmax.append(dEGGmax)\n",
    "\n",
    "egg_qci = []\n",
    "egg_dEGGmax = []\n",
    "for cycle in egg_cycles:\n",
    "    qci = find_qci(egg[cycle])\n",
    "    dEGGmax = find_dEGGmax(egg[cycle])\n",
    "    egg_qci.append(qci)\n",
    "    egg_dEGGmax.append(dEGGmax)\n",
    "\n",
    "# plot qci and dEGGmax\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(output_qci, label='Prediction QCI')\n",
    "plt.plot(egg_qci, label='Ground Truth QCI')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('EGG_Signal_Data.csv')\n",
    "\n",
    "# Extract columns\n",
    "sample_number = data['SampleNumber']\n",
    "true_egg = data['TrueEGG']\n",
    "predicted_egg = data['PredictedEGG']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(sample_number, true_egg, label='True EGG', color='green', alpha=0.6)\n",
    "plt.plot(sample_number, predicted_egg, label='Predicted EGG', color='red', alpha=0.6)\n",
    "plt.xlabel('Sample Number')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('EGG')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
