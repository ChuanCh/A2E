{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Conda\\envs\\HiFi\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerate = 16000\n",
    "\n",
    "# load data\n",
    "audio_path = r'F:\\Audio\\Audio'\n",
    "egg_path = r'F:\\Audio\\EGG'\n",
    "audio_list = os.listdir(audio_path)\n",
    "egg_list = os.listdir(egg_path)\n",
    "\n",
    "class AudioEGGDataset(Dataset):\n",
    "    def __init__(self, audio_path, egg_path, transform=None):\n",
    "        self.audio_path = audio_path\n",
    "        self.egg_path = egg_path\n",
    "        self.audio_list = os.listdir(audio_path)\n",
    "        self.egg_list = os.listdir(egg_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            audio_file = os.path.join(self.audio_path, self.audio_list[idx])\n",
    "            egg_file = os.path.join(self.egg_path, self.egg_list[idx])\n",
    "\n",
    "            # Load audio and EGG data\n",
    "            audio, sr = librosa.load(audio_file, sr=samplerate)  # None for native sampling rate, or replace with specific rate\n",
    "            egg, _ = librosa.load(egg_file, sr=samplerate)      # Assume same sample rate as audio\n",
    "\n",
    "            # Find the maximum length in the dataset or a predetermined max length\n",
    "            max_length = 160000  # This could also be dynamically calculated or set based on your data\n",
    "            # Pad or truncate to the maximum length\n",
    "            audio = librosa.util.fix_length(audio, size=max_length)\n",
    "            egg = librosa.util.fix_length(egg, size=max_length)\n",
    "\n",
    "            if self.transform:\n",
    "                audio = self.transform(audio)\n",
    "                egg = self.transform(egg)\n",
    "\n",
    "            # Convert to PyTorch tensors and add channel dimension\n",
    "            audio = torch.from_numpy(audio).float().unsqueeze(0)  # Add channel dimension\n",
    "            egg = torch.from_numpy(egg).float().unsqueeze(0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {audio_file} and {egg_file}: {e}\")\n",
    "            return None\n",
    "\n",
    "        return audio, egg\n",
    "\n",
    "dataset = AudioEGGDataset(audio_path, egg_path)\n",
    "# Create DataLoader\n",
    "batch_size = 2  # Adjust as necessary\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, input_channels, dilation_channels):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.receptive_field_size = 1\n",
    "        self.dilated_convs = nn.ModuleList()\n",
    "\n",
    "        # Create several dilated layers\n",
    "        dilations = [2**i for i in range(6)]  # Adjust as necessary\n",
    "        for dilation in dilations:\n",
    "            # Add proper padding to maintain input length\n",
    "            padding = dilation * 1  # Assuming kernel_size=2\n",
    "            self.dilated_convs.append(nn.Conv1d(input_channels, 2 * dilation_channels, kernel_size=2, padding=padding, dilation=dilation))\n",
    "            input_channels = dilation_channels  # After the first layer, input_channels should match dilation_channels\n",
    "\n",
    "        self.output_conv = nn.Conv1d(dilation_channels, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.dilated_convs:\n",
    "            out = conv(x)\n",
    "            # Splitting the output of the convolution into filter and gate parts\n",
    "            filter, gate = torch.split(out, self.dilation_channels, dim=1)  # Correct dimension for splitting is 1 (channels)\n",
    "            x = torch.tanh(filter) * torch.sigmoid(gate)\n",
    "\n",
    "        return self.output_conv(x)\n",
    "\n",
    "# Instantiate the model\n",
    "channels = 32  # You may need to tune this based on your dataset\n",
    "model = WaveNet(input_channels=1, dilation_channels=channels)\n",
    "\n",
    "\n",
    "# cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):  # Adjust the number of epochs based on your needs\n",
    "    for i, data in enumerate(dataloader):\n",
    "        audio, egg = data\n",
    "        audio = audio.to(device)\n",
    "        egg = egg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(audio)\n",
    "        loss = criterion(output, egg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Iteration {i}, Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
